\section{Discussion} \label{sec:discuss}
\subsection{Forward Model} \label{sec:forward-model}
In the previous section, we demonstrated the accuracy of \sedflow~posteriors. 
Nevertheless, a primary determining factor the fidelity of \sedflow, or any ML
model, is the quality of the training data set and, thus, the forward model
used to construct it. 
Below, we discuss the caveats and limitations of our forward model, which has
two components: the PROVABGS SPS model and noise model 
(Section~\ref{sec:training}).

First, for our noise model, we assign uncertainties to noiseless photometric
fluxes based on an empirical estimate of $p(\sigma_X\given f_X)$ for each band
independently. 
This is a simplicistic prescription and, as the bottom right panels of
Figure~\ref{fig:data} ($g - \sigma_r$ and $r - \sigma_r$) reveal, there are
discrepancies in the magnitude versus uncertainty distributions of the training
data and observations. 
Despite these discrepancies, \sedflow~provides excellent estimates of the true
posterior.  
This is because we design our ANPE to include $\sigma_X$ as a conditional
variable (Section~\ref{sec:anpe_train}).
The $f_X-\sigma_X$ distribution of our training data does not impact the
accuracy of the posteriors as long as there are sufficient training data near
$\bfi{x}$ to train the NDE in that region.

A more accurate noise model will, in theory, improve the performance of
\sedflow~because the $\bfi{x}$-space of the training data will more efficiently 
span the observations. 
Fewer training data would be expended in regions of $\bfi{x}$-space that are
devoid of observations.  
However, for our application, we do not find significantly  
improved performance when we alter the noise model.
This suggests that even with our simplistic noise model, the $\bfi{x}$-space of
observations is covered sufficiently well by the training data. 
We note that when we decrease $N_{\rm train}$ to below 500,000,
\sedflow~posteriors are significantly less accurate. 
A more realistic forward model may reduce this $N_{\rm train}$ threshold for
accurate posteriors. 
However, generating $N_{\rm train} \sim 1,000,000$ training has a negligible
computational cost compared to MCMC SED modeling, so we do not consider it
necessary to explore this further. 

Next, we consider limitations in the PROVABGS SPS model used in our forward
model. 
Our SPS model uses a compact and flexible prescription for SFH and ZH that can
describe a broad range of SFHs and ZHs.
However, the prescription is derived from simulated Illustris galaxies, whose
SFHs and ZHs may be not reflect the full range of SFHs and ZHs of real
galaxies.
If certain subpopulations of observed galaxies have SFHs and ZHs that cannot be
well described by the PROVABGS prescription, they cannot be accurately modeled.
Even if the PROVABGS SFH and ZH prescriptions are sufficient, there are
limitations in our understanding of stellar evolution. 

There is currently no consensus in the stellar evolution, stellar spectral
libraries, or IMF of galaxies~\citep[\emph{e.g.}][]{treu2010, vandokkum2010,
rosani2018, ge2019, sonnenfeld2019}.
The PROVABGS model uses MIST isochrones, \cite{chabrier2003} IMF, and the MILES
+ BaSeL spectral libraries. 
These choices limit the range of SEDs that can be produced by the training
data. 
For instance, if galaxies have significant variations in their IMF, assuming a
fixed IMF would falsely limit the range of our training data.  
A more flexible SED model that includes uncertainties in SPS would broaden the
range of galaxy SEDs that can be modeled.
Data-driven approaches may also enable SED models to be more
descriptive~\citep[\emph{e.g.}][]{hogg2016, portillo2020}. 
Improving  SED models, however, is beyond the scope of this work. 
Our focus is on improving the Bayesian inference framework.
In that regard, the limitations of the SED model equally impacts conventional
approaches with MCMC. 

We encounter the caveats above when we apply \sedflow~to the NSA catalog. 
For a small fraction of NSA galaxies (591 out of 33,883), \sedflow~generates
posteriors that are outside of the prior volume. 
This is because the photometry or uncertainties of these galaxies lie outside
of the support of the training data where \sedflow~is not trained. 
They either have higher photometric uncertainties, for a given magnitude, or
bluer photometric colors than the training data. 
Some of these may be observational artifacts or problematic photometry.
Nevertheless, \sedflow~fails because we cannot construct training data near
them in our limited noise and SPS models. 
Since this only affects a small fraction of the NSA galaxies, we flag them in
our catalog. 
We infer their galaxy properties by applying PROVABGS with MCMC sampling for
completeness.
For more details on these failures, we refer readers to
Appendix~\ref{sec:fail}.

To test for limitations of the forward model, we can construct additional tests
of posteriors derived from ANPE. 
For instance, the $\chi^2$ of the best-fit parameter value from the 
estimated posterior can be used to assess whether the best-fit model 
accurately reproduces observations.
This would only require one additional model evaluation per galaxy. 
One can also construct an Amortized Neural Likelihood Estimator (ANLE) using
the same training data.
Unlike the ANPE, which estimates $p(\btheta\given f_X, \sigma_X, z)$, the ANLE
would estimate $p(f_X \given \btheta, \sigma_X, z)$.
We can then further validate the posteriors by assessing whether the observed
photometry lies within the ANLE distribution. 
Based on the overall high level of accuracy of \sedflow~posteriors, we do not
explore these additional tests; however, they can be used to further validate
any ANPE posteriors. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Advantages of \sedflow} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The primary advantage of \sedflow~is its computational speed. 
This becomes even more relevant if we want to add additional parameters to
address the concerns about the choices in current SPS models we described
above.
To relax these assumptions, SPS models would need to introduce additional
parameters that flexibly model these uncertainties~\citep{conroy2009,
conroy2010c}. 
While the dimensionality of current SPS models already makes MCMC methods
computational infeasible, ANPE has already been applied to higher dimensional
applications.
For instance, \cite{dax2021} constructed an accurate ANPE for a
15-dimensional model parameter space and 128-dimensional conditional variable
space.
NDE is an actively developing field in ML and new methods are constantly
emerging~\citep[\eg][]{wu2020, dhariwal2021}. 
Since ANPE can handle higher dimensionality, we can in the future include
additional parameters that model uncertainties in SPS. 
This will not only improve our SED modeling, but also improve our understanding
of stellar evolution and the IMF.

In addition to enabling scalable SED modeling for the next generation galaxy
surveys, \sedflow~will enable us to tackle other key challenges in SED
modeling. 
For example, recent works have demonstrated that priors of SED models can
significantly impact the inferred galaxy properties~\citep{carnall2018,
leja2019, hahn2022}. 
Even ``uniformative'' uniform priors on SED model parameters can impose
undesirable priors on derived galaxy properties such as $M_*$, SFR, SFH, or
ZH.
To avoid significant biases, galaxy studies must carefully select priors and
validate their results using multiple different choices. 
With an MCMC approach, selecting a different prior means reevaluating every
posterior and repeating all the SED model evaluations in the MCMC sampling.  

%\sedflow~requires substantially fewer model evaluations overall; however, the
%advantages extend even further. 
For an ANPE approach, the prior is set by the distribution of parameters in the
training data. 
For a new prior, instead of reconstructing the training data, we can resample
it in such a way that the parameters follow the new prior.
Then, the ANPE model can be re-trained, re-validated on the test data, and
re-deployed on observations.
Each of these steps require substantially less computational resources than
generating a new set of training data or using MCMC methods. 
Hence, the ANPE approach provides a way to efficiently vary the prior without
multiplying computational costs.
