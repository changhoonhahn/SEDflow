\section{Discussion} \label{sec:discuss}
\subsection{Further Validation} 
In the previous section, we validated and demonstrated the accuracy of
\sedflow~posteriors. 
Nevertheless, a primary determining factor of \sedflow, and any ML model, is
the quality of the training data.
In our case, the training data is generated using a forward model with two
components: the PROVABGS SED model and a noise model
(Section~\ref{sec:training}).
We first consider the noise model. 

In our noise model, we assign noiseless photometric fluxes uncertainties based
on an empirical estimate of $p(\sigma_X\given f_X)$ for each band
independently. 
Afterwards, the assigned $\sigma_X$ is used to apply Gaussian noise to the
photometric flux (Eq.~\ref{eq:noise}). 
This is a simplicistic noise model and, as the bottom right ($g - \sigma_r$ and
$r - \sigma_r$) panels of Figure~\ref{fig:data} reveal, there are discrepancies
in the magnitude versus uncertainty distributions of the training data and
observations. 
Despite these discrepancies, \sedflow~provides excellent estimates of the true
posterior.  
This is because we design our ANPE to include $\sigma_X$ as a conditional
variable (Section~\ref{sec:anpe_train}).
The $f_X-\sigma_X$ distribution of our training data does not impact the
accuracy of the posteriors as long as there is sufficient training data near
$\xobs$ to properly train the NDE in the region.

A more accurate noise model will, in theory, improve the performance of
\sedflow~since the $\bfi{x}$-space of the training data will more effectively
span the observations. 
In other words, there will be fewer training data expended in regions of 
${\bf x}$-space that are devoid of observations.  
However, for our application to SED modeling, we do not find significantly  
improved performance when we replace the noise model.
This suggests that even with our simplicistic forward model, the
$\bfi{x}$-space of observations is sufficiently covered by the training data. 
We note that when we decrease $N_{\rm train}$ below 500,000,
\sedflow~posteriors are significantly less accurate. 
A more realistic forward model may reduce this $N_{\rm train}$ threshold for
accurate posteriors. 
However, generating $N_{\rm train} \sim 1,000,000$ training has a negligible
computational cost compared to MCMC SED modeling so we do not explore this
further. 

Next, we consider limitations in the PROVABGS SED model used in our forward
model. 
One of its advantages is that it uses a compact and flexible prescription for
SFH and ZH that can describe a broad range of SFHs and ZHs.
However, the prescription was derived from simulated Illustris galaxies, whose
SFHs and ZHs may be not reflect the full range of SFHs and ZHs of real galaxies.
If certain subpopulations of galaxies have SFHs and ZHs that cannot be
described by the PROVABGS prescriptions, they cannot be accurately forward
modeled. 
Furthermore, even if the PROVABGS SFH and ZH prescriptions are sufficient,
there are limitations in our understanding of stellar evolution. 

There is currently no consensus in the stellar evolution, stellar spectral
libraries, or IMF of galaxies~\citep[\emph{e.g.}][]{treu2010, vandokkum2010,
rosani2018, ge2019, sonnenfeld2019}.
The PROVABGS model uses MIST isochrones, \cite{chabrier2003} IMF, and the MILES
+ BaSeL spectral libraries. 
These choices limit the range of SED that can be produced by the training data. 
For instance, if galaxies have significant variations in their IMF, assuming a
fixed IMF would reduce the range of our training data.  
A more flexible SED model that includes uncertainties in SPS would broaden the
range of galaxy SEDs that can be modeled.
Data-driven approaches may also enable SED models to be more
descriptive~\citep[\emph{e.g.}][]{hogg2016, portillo2020}. 
Improving  SED models, however, is beyond the scope of this work. 
Our focus is on improving the Bayesian inference framework.
In that regard, the limitations of the SED model equally impacts conventional
approaches with MCMC. 

%\todo{revisit paragraph below after applying sedflow to NSA} 
%Although we demonstrate that \sedflow~produces accurate posteriors for $\xobs$
%within the $\bfi{x}$ support of the training data, there are a few objects in
%the NSA catalog are outside of the support. 
%For instance, the $g$, $r$, $\sigma_r$ panels of Figure~\ref{fig:data} reveal a
%number of NSA objects (blue) that lie outside of the $g-r$ color distribution
%of the training data (black). 
%This is not due to deficiencies in the noise model since the $g-\sigma_r$ and
%$r-\sigma_r$ relations of these NSA objects are within that of the training 
%data. 
%Some of these objects are likely observational artifacts. 
%Even with the quality cuts in Section~\ref{sec:obs}, there are likely objects
%in our samples with problematic photometry.  
%\chedit{list a few typical examples of problematic photometry}. 
%The SEDs of such artifcats cannot be modeled using an SPS model combined with
%noise, so they can lie outside of the training ${\bf x}$ support.

%In Section~\ref{sec:results}, we assessed the accuracy of the \sedflow~posteriors using posteriors derived using MCMC and test observations, where we know the true parameter values. 
%Both cases demonstrate the high level of accuracy of \sedflow.  However, for applications that require even higher fidelity posteriors, there are further tests. 
Given the limitations of any forward model, we can construct additional tests
of posteriors derived from ANPE. 
For instance, the $\chi^2$ of the best-fit parameter value from the 
estimated posterior can be used to test whether the best-fit model 
accurately reproduces observations.
This would only require one additional model evaluation. 
As another test of the posteriors, one can construct an Amortized Neural
Likelihood Estimator (ANLE) using the same training data. 
Unlike the ANPE, which estimates $p(\theta\given f_X, \sigma_X, z)$, the ANLE
would estimate $p(f_X \given \theta, \sigma_X, z)$.
We can then further validate the posteriors by assessing whether the observed
photometry lies within the ANLE. 
Based on the high level of accuracy of \sedflow~posteriors in the previous
section, we do not explore these additional tests; however, they can be used to
further validate any ANPE posterior. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Additional Advantages} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The primary advantage of \sedflow~is its computational speed. 
It accelerates Bayesian inference to make SED modeling scalable for the
billions of galaxies that will be observed by upcoming galaxy surveys. 
Furthermore, by removing the computational bottleneck of Bayesian inference,
\sedflow~enables us to tackle other key challenges in SED modeling.  

For instance, recent works have demonstrated that priors of SED models play a
crucial role in the derived galaxy properties~\citep{carnall2018, leja2019,
hahn2022}. 
Even ``uniformative'' uniform priors on SED model parameters can impose
undesirable priors on derived galaxy properties such as $M_*$, SFR, SFH, or
ZH.
To avoid significant biases from this effect, galaxy studies must carefully
select priors and validate their results using multiple different choices. 
For SED modeling with MCMC, selecting a different prior requries reevaluating
every posterior and repeating all the SED model evaluations in the MCMC
sampling.  

The ANPE approach, overall, requires substantially fewer model evaluations; 
however, the advantages extend even further. 
For ANPE, the prior is set by the distribution of parameters in the training
data. 
For a new prior, instead of regenerating the training data, we can subsample
it so that the parameters follow the new prior. 
Then, the ANPE model can be re-trained, re-validated on the test data, and
re-deployed on observations.
Each of these steps require substantially less computational resources than
generating a new set of training data or using MCMC methods. 
Hence, the ANPE approach provides a way to efficiently vary the prior without
multiplying computational costs.

Another challenge in SED modeling is that current SPS models make strong
theoretical assumptions.
As mentioned above, they do not account for uncertainties in stellar evolution,
spectral libraries, or the IMF. 
To relax these assumptions, SPS models would need to introduce additional
parameters that flexibly model these uncertainties. 
While the dimensionality of current SPS models already makes MCMC methods
computational infeasible, ANPE has been applied to higher dimensional
applications.
\cite{dax2021}, for instance, constructed an accurate ANPE for a
15-dimensional model parameter space and 128-dimensional conditional variable
space.
NDE is an actively developing field in ML and new methods are constantly
emerging~\citep[\eg][]{wu2020, dhariwal2021}. 
Since ANPE can handle higher dimensionality, we can include additional
parameters that model uncertainties in SPS. 
This will not only improve our SED modeling, but also improve our understanding
of stellar evolution and the IMF. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Next Applications} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this work, we apply the ANPE approach to optical photometry, photometric
uncertainties, and redshifts from the NSA. 
Our approach can easily be extended beyond this application. 
For instance, we can include multi-wavelength photometry at ultra-violet (UV)
or infrared (IR) wavelengths. 
We can also modify \sedflow~to infer redshift from photometry. 
In \sedflow, we include redshift as a conditional variable, since NSA provides
spectroscopic redshifts. 
However, redshift can be included as an inferred variable rather than a
conditional one. 
Then, we can apply \sedflow~to infer galaxy properties from photometric data
sets without redshift measurements while marginalizing over the redshift
prior. 
If we do not require spectroscopic redshifts, \sedflow~can be extended to much
larger data sets that span fainter and broader galaxy samples. 
Conversely, we can use \sedflow~to infer more physically motivated photometric 
redshifts, where we marginalize over our understanding of galaxies rather than
using templates. 

The ANPE approach to SED modeling can also be extended to galaxy spectra. 
Constructing an ANPE for the full data space of spectra, however, requires
estimating a dramatically higher dimensional probability distribution. 
SDSS spectra, for instance, have ${\sim}3,600$ spectral elements.  
Furthermore, in our approach we include the uncertainties of observables as
conditional variables, which double the curse of dimensionality.
Recent works, however, have demonstrated that galaxy spectra can be represented
in a compact low-dimensional space using autoencoders~\citep[][; Melchior \&
Hahn in prep.]{portillo2020}.
In \cite{portillo2020}, they demonstrate that SDSS galaxy spectra can be
compressed into 7-dimensional latent variable space with little loss of
information. 
Such spectral compression dramatically reduces the dimensionality of the
conditional variable space to dimensions that can be tackled by current ANPE
methods. 
We will explore SED modeling of galaxy spectrophotometry using ANPE and
spectral compression in a following work. 

With \sedflow, we can generate accurate posteriors for SED modeling >$10^4$
faster than conventional MCMC methods. 
Analyzing 33,887 NSA galaxies takes <12 CPU hours.
Analyzing the 10 million galaxies of DESI BGS would only take ${\sim}3,000$ CPU
hours.
An ANPE approach will enable rigorous SED modeling on the enormous galaxy
samples that will be observed by DESI, PFS, Rubin, JWST, and Roman. 
The Rubin observatory, alone, will soon provide photometry from billions of
galaxies. 
We will be able to construct probabilistic catalogs that provide accurate
posteriors on the physical properties for all of these galaxies. 

%In addition to accurately measuring galaxy properties, these probabilistic catalogs will enable new class of statistical techniques.  For instance, they can be used for population inference to derive the distribution of galaxy properties for galaxy populations by combining individual posteriors~\citep[\eg][]{leja2019a}.  With probabilistic catalogs, we can also robustly probe less explored, low signal-to-noise, regimes that may shed new light on galaxy evolution, \eg~dwarf galaxies.  Since posteriors accurately quantify uncertainty, analyses can include galaxies with less tightly constrained properties without introducing bias.  We can also more reliably quantify the extreme or outlier galaxies.  These catalogs can also take advantage of Bayesian Hierarchical approaches that improve the statistical power of observations through Bayesian shrinkage.  Overall, these probabilistic catalogs will enable a new level of statistical robustness in galaxy studies and more fully extract the statistical power of future observations. 
