\section{Simulation-Based Inference} \label{sec:sbi}
% standard bayesian approach and introducing SBI
The goal of Bayesian SED modeling, and probabilistic inference more
broadly, is to infer the posterior probability distributions
$p(\btheta\given\bfi{x})$ of galaxy properties, $\btheta$, given observations, 
$\bfi{x}$.
For a specific $\btheta$ and $\bfi{x}$, we can evaluate the posterior using
Bayes' rule, 
$p(\btheta\given\bfi{x}) \propto p(\btheta)~p(\bfi{x}\given\btheta)$, where 
$p(\btheta)$ denotes the prior distribution and $p(\bfi{x}\given\btheta)$ the
likelihood, which is typically assumed to have a Gaussian functional form: 
\beq
\label{eq:likelihood}
    \ln p(\bfi{x}\given\btheta) = -\frac{1}{2}\left(\bfi{x} - m(\btheta)\right)^T {\bf C}^{-1}
    \left(\bfi{x} - m(\btheta)\right).
\eeq
$m(\btheta)$ is the theoretical model, in our case a galaxy SED model from SPS.
${\bf C}$ is the covariance matrix of the observations. 
In practice, off-diagonal terms are often ignored and measured uncertainties
are used as estimates of the diagonal terms. 

% overview of SBI and mention of ABC
Simulation-based inference (SBI; also known as ``likelihood-free'' inference)
offers an alternative that requires no assumptions about the form of the
likelihood. 
Instead, SBI uses a generative model, \emph{i.e.} a simulation $F$, to generate
mock data $\bfi{x}'$ given parameters $\btheta'$: $F(\btheta') = \bfi{x}'$. 
It uses a large number of simulated pairs $(\btheta', \bfi{x}')$ to directly estimate
either the posterior  $p(\btheta\given \bfi{x})$, the likelihood
$p(\bfi{x}\given \btheta)$, or the joint distribution of the parameters and data $p(\btheta, \bfi{x})$. 
SBI has already been successfully applied to a number of Bayesian parameter
inference problems in astronomy~\citep[\emph{e.g.}][]{cameron2012, weyant2013,
hahn2017b, kacprzak2018, alsing2018, wong2020, huppenkothen2021, zhang2021}
and in physics~\citep[\emph{e.g.}][]{brehmer2019, cranmer2020}.

%One simple and pedagogical example of SBI is Approximate Bayesian Computation~\citep[ABC;][]{rubin1984, pritchard1999, beaumont2002}, which uses a rejection sampling framework to estimate the posterior.  First, parameter values are sampled from the prior: $\btheta'\sim p(\btheta)$.  The forward model, $F$, is then run on $\btheta'$ to generate simulated data $F(\btheta') = \bfi{x}'$.  If the simulated $\bfi{x}'$ is `close' to the observed $\bfi{x}$, usually based on a threshold on some distance criterion $\rho(\bfi{x}', \bfi{x}) < \epsilon$, $\btheta'$ is kept.  Otherwise, $\btheta'$ is rejected.  This process is repeated until there are enough samples to estimate the posterior.  The estimated posterior from ABC can be written as $p(\btheta \given \rho(F(\btheta), \bfi{x}) < \epsilon)$.  In the case where $\epsilon\rightarrow 0$, the conditional statement is equivalent to the condition $F(\btheta) = \bfi{x}$; thus, the estimated ABC posterior is  equivalent to the true posterior: $p(\btheta \given \rho(F(\btheta), \bfi{x}) < \epsilon\rightarrow 0) \equiv p(\btheta \given \bfi{x})$.


\subsection{Amortized Neural Posterior Estimation} \label{sec:flow}
SBI provides another a critical advantage over MCMC inference methods --- it
enables \emph{amortized inference}. 
For SED modeling using MCMC, each galaxy requires >$10^5$ model evaluations to
accurately estimate $p(\btheta \given \bfi{x})$~(\citealt{hahn2022}, 
Kwon~\etal~in prep.).
Moreover, model evaluations for calculating the posterior of one galaxy cannot
be used for another. 
This makes MCMC approaches for SED modeling of upcoming surveys computationally
infeasible.

With density estimation SBI, we require a large number (${\sim}10^6$) of model
evaluations only initially to train a neural density estimator (NDE), 
\emph{i.e.} a neural network with parameters $\bphi$ that is trained to
estimate the density $p_\phi(\btheta \given \bfi{x}')$.
If the training covers the entire or the practically relevant portions of the
$\btheta$ and $\bfi{x}$ spaces, we can evaluate
$p_\phi(\btheta\given\bfi{x}_i)$ for each galaxy $i$ with minimal computational
cost. 
The inference is therefore amortized and no additional model evaluations are
needed to generate the posterior for each galaxy.
This technique is called  Amortized Neural Posterior Estimation (ANPE) 
and has recently been applied to a broad range of astronomical applications
from analyzing gravitational waves~\citep[\emph{e.g.}][]{wong2020,dax2021} to
binary microlensing lensing~\citep{zhang2021}.
For SED modeling, the choice in favor of using ANPE is easy: the entire upfront
cost for ANPE model evaluations would only yield posteriors of tens of galaxies
with MCMC.

ANPE makes two important assumptions.
First, the simulator $F$ is capable of generating mock data $\bfi{x}'$ that is
practically indistinguishable from the observations.
In terms of the expected signal, $m$ in Eq.~\ref{eq:likelihood}, this is the
same requirement as any probabilistic modeling approach. 
But unlike likelihood-based evaluations, such as conventional MCMC, data
generated for SBI need to include all relevant noise terms as well. 
We address both aspects in Sections \ref{sec:training} and \ref{sec:forward-model}.
Second, ANPE assumes that the NDE is trained well: 
$p_\phi(\btheta \given \bfi{x}')$ is a good approximation of 
$p(\btheta \given \bfi{x}')$, and therefore of $p(\btheta \given \bfi{x})$. 
We assess this in Section~\ref{sec:results}.


ANPE commonly employs so-called ``normalizing flows''~\citep{tabak2010,
tabak2013} as density estimators.
Normalizing flow models use an invertible bijective transformation, $f$, to map
a complex target distribution to a simple base distribution, $\pi(\bfi{z})$, that is
fast to evaluate.
For ANPE, the target distribution is $p(\btheta \given \bfi{x})$ and the
$\pi(\bfi{z})$ is typically a simple multivariate Gaussian, or mixture of Gaussians.
The transformation $f: \bfi{z} \rightarrow \btheta$ must be invertible and have a
tractable Jacobian. 
This is so that we can evaluate the target distribution from $\pi(\bfi{z})$ by
a change of variable:  
\begin{equation} \label{eq:normflow}
    p(\btheta \given {\bf x}) = \pi(\bfi{z}) \Bigl|{\rm det} \left(\frac{\partial
    f^{-1}}{\partial \btheta} \right)\Bigr|.
\end{equation} 
Since the base distribution is easy to evaluate, we can also easily evaluate
the target distribution.  
A neural network is trained to obtain $f$, the collection of its parameters form $\bphi$.
The network typically consists of a series of simple transforms (\emph{e.g.}
shift and scale transforms) that are each invertible and whose Jacobians are
easily calculated. 
By stringing together many such transforms, $f$ provides an extremely flexible
mapping from the base distribution.
%Rather than a single complicated transformation, the network is typically restricted to a series of simple transforms that are each invertible and whose Jacobians are easily calculated. 

Many different normalizing flow models are now available in the
literature~\citep[\emph{e.g.}][]{germain2015, durkan2019}.
In this work, we use Masked Autoregressive
Flow~\citep[MAF;][]{papamakarios2017}. 
The autoregressive design~\citep{uria2016} of MAF is particularly well-suited
for modeling conditional probability distributions such as the posterior. 
Autoregressive models exploit chain rule to expand a joint probability of a set
of random variables as products of one-dimensional conditional
probabilities: $p(\bfi{x}) = \prod_i p(x_i\given x_{1:i-1})$. 
They then use neural networks to describe each conditional probability,
$p(x_i\given x_{1:i-1})$. 
In this context, we can add a conditional variable $y$ on both sides of the
equation, $p(\bfi{x}\given \bfi{y}) = \prod_i p(x_i\given x_{1:i-1}, \bfi{y})$, so that the
autoregressive model describes a conditional probability $p(\bfi{x}\given \bfi{y})$. 
One drawback of autoregressive models is their sensitivity to the ordering of
the variables. 
Masked Autoencoder for Distribution Estimation~\citep[MADE;][]{germain2015}
models address this limitation using binary masks to impose the autoregressive
dependence and by permutating the order of the conditioning variables.
A MAF model is built by stacking multiple MADE models.  
Hence, it has the autoregressive structure of MADE but with more flexibility to
describe complex probability distributions.  
In practice, we use the MAF implementation in the $\mathtt{sbi}$ Python
package\footnote{\url{https://github.com/mackelab/sbi/}}~\citep{greenberg2019,
tejero-cantero2020}.
