\section{Simulation-Based Inference} \label{sec:sbi}
% standard bayesian approach and introducing SBI
The ultimate goal of Bayesian SED modeling, and probabilistic inference more
broadly, is to infer the posterior probability distributions of galaxy
properties, $\theta$, given observations, $\xobs$  --- $p(\theta\given\xobs)$.
We can evaluate the posterior at a specific $\theta$ and ${\bf x}$ using
Bayes' rule, $p(\theta\given\xobs) \propto p(\theta)~p(\xobs\given\theta)$. 
$p(\theta)$ is the prior distribution, which we specify. 
And $p(\xobs\given\theta)$ is the likelihood, which is {\em typically}
evaluated using a surrogate Gaussian functional form: 
\beq
    \ln p(\xobs\given\theta) = -\frac{1}{2}(\xobs - m(\theta))^t {\bf C}^{-1}
    (\xobs - m(\theta)).
\eeq
$m(\theta)$ is the theoretical model, in our case a galaxy SED model from
stellar population synthesis.
${\bf C}$ is the covariance matrix of the observations. 
In practice, off-diagonal terms are often ignored and measured are
uncertainties are used as estimates of the diagonal terms. 

In the standard approach, the full posterior distribution is derived by
evaluating the posterior with a sampling technique such as Markov Chain Monte
Carlo (MCMC) or nested sampling~\citep[\eg][]{carnall2017, leja2019a,
tacchella2021}.
These sampling techniques are essential for the efficient exploration of
the relatively higher dimensionality of SED model parameter space.
Even advanced techniques, however, are subject to major limitations.  
For instance, MCMC sampling techniques can struggle to accurately estimate
multimodal and degenerate posteriors. 
Many also require significant hand-tuning by the user.
More importantly, despite their efficiency, these techniques require on the
order of a {\em million} SED model evaluations to derive a posterior --- this
can take ${\sim}100$ of CPU hours per galaxy.
Analyzing the tens of millions of spectra or billions of photometry from
upcoming surveys (\eg~DESI, Rubin, Roman) with these approaches would thus
require {\em billions of CPU hours}.

% overview of SBI and mention of ABC
Simulation-based inference (SBI; also known as ``likelihood-free'' inference)
offers a more scalable approach to Bayesian SED modeling.
At its core, SBI involves any method that uses a forward model of the observed
data to directly estimate the posterior ($p(\theta\given\xobs)$), the
likelihood ($p({\bf x}\given \theta$), or the joint distribution of the
parameters and data ($p(\theta, {\bf x})$). 
SBI methods have already been successfully applied to a number of Bayesian
parameter inference problems in astronomy~\citep[\emph{e.g.}][]{cameron2012, 
weyant2013, hahn2017b, kacprzak2018, alsing2018, wong2020, huppenkothen2021,
zhang2021}, and more broadly in physics~\citep[\emph{e.g.}][]{brehmer2019,
cranmer2020}

One simple example of SBI is Approximate Bayesian
Computation~\citep[ABC;][]{rubin1984, pritchard1999, beaumont2002}, which uses
a rejection sampling framework to estimate the posterior. 
First, parameter values are sampled from the prior: $\theta'\sim p(\theta)$. 
The forward model, $F$ is then run on the sampled $\theta'$ to generate
simulated data $F(\theta') = {\bf x}'$.
If the simulated ${\bf x}'$ is `close' to the observed $\xobs$, usually based
on a threshold on some distance metric $\rho({\bf x}', \xobs) < \epsilon$, 
$\theta'$ is kept. 
Otherwise, $\theta'$ is rejected. 
This process is repeated until there are enough samples to estimate the
posterior. 
The estimated posterior from ABC can be written as 
$p(\theta \given \rho(F(\theta), \xobs) < \epsilon)$. 
In the case where $\epsilon\rightarrow 0$, the conditional statement is
equivalent to the condition $F(\theta) = \xobs$; thus, the estimated ABC
posterior is {\em equivalent} to the true posterior:
$p(\theta \given \rho(F(\theta), \xobs) < \epsilon\rightarrow 0) \equiv
p(\theta \given \xobs)$.

ABC produces unbiased estimates of the posterior and only requires a forward
model of the observed data.
It makes no assumptions on the likelihood and, therefore, relaxes the
assumptions that go into surrogate likelihood methods. 
However, despite its advantages, ABC has one major limitation --- it is
computational inefficient. 
Rejection sampling means it 


New SBI methods, such as density estimation SBI~\citep[\eg][]{papamakarios2017,
alsing2018, hahn2019c, greenberg2019, tejero-cantero2020},

%Density estimation likelihood-free inference works by learn- ing a parameterized model for the joint density P(θ, d), from a set of samples drawn from that density (Papamakarios & Murray 2016). In its simplest form, we start by generating a set of samples {θ, d} from P(θ, d) by drawing parameters from the prior and forward simulating mock data:
%θ ← P(θ)
%d ← P(d|θ). (4)
%We then write down a model for the joint density P(θ, d; η), parameterized by η, and fit this model to the samples {θ, d}. The estimated2 posterior density and Bayesian evidence can then be easily extracted from the fit to the joint density as follows:
%Pˆ(θ|do) ∝ P(θ,d = do;η)
%Pˆ(do) =
%∫
%P(θ, d = do; η) dθ, (5)
%ie., taking a slice through the joint distribution evaluated at the observed data d = do, and subsequently integrating over the parameters for the Bayesian evidence. For many prac- tical choices of parameterized models for the joint density, eg., Gaussian mixture models (see below), the evidence in- tegral in Eq. (5) is analytically tractable. This means that the evidence comes for free, and if the parameterized model for the joint density is fit to the samples in a principled way, the uncertainties on the fit parameters can be propa- gated through to a principled uncertainty on the estimated Bayesian evidence.
%In contrast to abc, delfi uses all of the available for- ward simulations {θ,d} to inform the inference of the joint density P(θ, d), and hence the posterior density and evidence estimation. In practice, this means that far fewer forward simulations may be needed to obtain high-fidelity posterior inferences (compared to abc that has a vanishingly small ac- ceptance rate as ε → 0), as demonstrated by Papamakarios & Murray (2016).




\subsection{Amortized Neural Posterior Estimation} \label{sec:flow}
% section explaining conditional normalizing flows and how we condition on the

% citation for SBI package 
sbi~\citep{greenberg2019, tejero-cantero2020}

% examples of ANPE 
\cite{wong2020}
\cite{dax2021}
\cite{zhang2021}

\begin{itemize}
    \item example normalizing flows and MAF 
\end{itemize}
