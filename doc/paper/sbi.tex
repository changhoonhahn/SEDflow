\section{Simulation-Based Inference} \label{sec:sbi}
% standard bayesian approach and introducing SBI
The ultimate goal of Bayesian SED modeling, and probabilistic inference more
broadly, is to infer the posterior probability distributions of galaxy
properties, $\theta$, given observations, $\xobs$  --- $p(\theta\given\xobs)$.
We can evaluate the posterior at a specific $\theta$ and ${\bf x}$ using
Bayes' rule, $p(\theta\given\xobs) \propto p(\theta)~p(\xobs\given\theta)$. 
$p(\theta)$ is the prior distribution, which we specify. 
And $p(\xobs\given\theta)$ is the likelihood, which is {\em typically}
evaluated using a surrogate Gaussian functional form: 
\beq
    \ln p(\xobs\given\theta) = -\frac{1}{2}(\xobs - m(\theta))^t {\bf C}^{-1}
    (\xobs - m(\theta)).
\eeq
$m(\theta)$ is the theoretical model, in our case a galaxy SED model from
stellar population synthesis.
${\bf C}$ is the covariance matrix of the observations. 
In practice, off-diagonal terms are often ignored and measured are
uncertainties are used as estimates of the diagonal terms. 

In the standard approach, the full posterior distribution is derived by
evaluating the posterior with a sampling technique such as Markov Chain Monte
Carlo (MCMC) or nested sampling~\citep[\eg][]{carnall2017, leja2019a,
tacchella2021}.
These sampling techniques are essential for the efficient exploration of
the relatively higher dimensionality of SED model parameter space.
Even advanced techniques, however, are subject to major limitations.  
For instance, MCMC sampling techniques can struggle to accurately estimate
multimodal and degenerate posteriors. 
Many also require significant hand-tuning by the user.
More importantly, despite their efficiency, these techniques require on the
order of a {\em million} SED model evaluations to derive a posterior --- this
can take ${\sim}100$ of CPU hours per galaxy.
Analyzing the tens of millions of spectra or billions of photometry from
upcoming surveys (\eg~DESI, Rubin, Roman) with these approaches would thus
require {\em billions of CPU hours}.

% overview of SBI and mention of ABC
Simulation-based inference (SBI; also known as ``likelihood-free'' inference)
offers a more scalable approach to Bayesian SED modeling.
At its core, SBI involves any method that uses a forward model of the observed
data to directly estimate the posterior ($p(\theta\given\xobs)$), the
likelihood ($p({\bf x}\given \theta$), or the joint distribution of the
parameters and data ($p(\theta, {\bf x})$). 
SBI methods have already been successfully applied to a number of Bayesian
parameter inference problems in astronomy~\citep[\emph{e.g.}][]{cameron2012, 
weyant2013, hahn2017b, kacprzak2018, alsing2018, wong2020, huppenkothen2021,
zhang2021}, and more broadly in physics~\citep[\emph{e.g.}][]{brehmer2019,
cranmer2020}

One simple and pedagogical example of SBI is Approximate Bayesian
Computation~\citep[ABC;][]{rubin1984, pritchard1999, beaumont2002}, which uses
a rejection sampling framework to estimate the posterior. 
First, parameter values are sampled from the prior: $\theta'\sim p(\theta)$. 
The forward model, $F$ is then run on the sampled $\theta'$ to generate
simulated data $F(\theta') = {\bf x}'$.
If the simulated ${\bf x}'$ is `close' to the observed $\xobs$, usually based
on a threshold on some distance metric $\rho({\bf x}', \xobs) < \epsilon$, 
$\theta'$ is kept. 
Otherwise, $\theta'$ is rejected. 
This process is repeated until there are enough samples to estimate the
posterior. 
The estimated posterior from ABC can be written as 
$p(\theta \given \rho(F(\theta), \xobs) < \epsilon)$. 
In the case where $\epsilon\rightarrow 0$, the conditional statement is
equivalent to the condition $F(\theta) = \xobs$; thus, the estimated ABC
posterior is {\em equivalent} to the true posterior:
$p(\theta \given \rho(F(\theta), \xobs) < \epsilon\rightarrow 0) \equiv
p(\theta \given \xobs)$.

ABC produces unbiased estimates of the posterior and only requires a forward
model of the observed data.
It makes no assumptions on the likelihood and, therefore, relaxes the
assumptions that go into surrogate likelihood methods. 
Nevertheless, ABC is based on rejection sampling and thus requires comparable
number of model evaluations as standard MCMC sampling based techniques. 
ABC is only the simplest SBI method; new SBI methods can infer posteriors with
much fewer model evaluations.
Density estimaiton-based SBI methods~\citep[\eg][]{papamakarios2017,
alsing2018, hahn2019c, greenberg2019, tejero-cantero2020}, for instance, use
model evaluations to fit estimates of $p(\theta\given\xobs)$, $p({\bf
x}\given \theta$, or $p(\theta, {\bf x})$ probability distributions.  
They can exploit recent advances in neural density estimation (NDE) that
increasingly enable high-fidelity density estimation with fewer samples of the
distribution.
For instance, the NDE in \cite{papamakarios2017} accurately estimates the
$28\times28=784$-dimensional distribution of the MNIST
dataset\footnote{http://yann.lecun.com/exdb/mnist/} with only tens of thousands
of samples. 

\subsection{Amortized Neural Posterior Estimation} \label{sec:flow}
Density estimation SBI provides a key advantage over MCMC sampling-based
inference methods --- it enables \emph{amortized inference}. 

% section explaining conditional normalizing flows and how we condition on the

% citation for SBI package 
sbi~\citep{greenberg2019, tejero-cantero2020}

% examples of ANPE 
\cite{wong2020}
\cite{dax2021}
\cite{zhang2021}

\begin{itemize}
    \item example normalizing flows and MAF 
\end{itemize}
