\section{Simulation-Based Inference} \label{sec:sbi}
% standard bayesian approach and introducing SBI
The goal of Bayesian SED modeling, and probabilistic inference more
broadly, is to infer the posterior probability distributions $p(\btheta\given\bfi{x})$
of galaxy properties, $\btheta$, given observations $\bfi{x}$.
For a specific $\btheta$ and $\bfi{x}$, we can evaluate the posterior using
Bayes' rule, $p(\btheta\given\bfi{x}) \propto p(\btheta)~p(\bfi{x}\given\btheta)$,  
where $p(\btheta)$ denotes the prior distribution and
$p(\bfi{x}\given\btheta)$ the likelihood, which is typically assumed to have a
Gaussian functional form: 
\beq
\label{eq:likelihood}
    \ln p(\bfi{x}\given\btheta) = -\frac{1}{2}(\bfi{x} - m(\btheta))^T {\bf C}^{-1}
    (\bfi{x} - m(\btheta)).
\eeq
$m(\btheta)$ is the theoretical model, in our case a galaxy SED model from SPS.
${\bf C}$ is the covariance matrix of the observations. 
In practice, off-diagonal terms are often ignored and measured uncertainties
are used as estimates of the diagonal terms. 

%In the standard approach, the full posterior distribution is estimated by exploring the posterior with a sampling technique such as Markov Chain Monte Carlo~\citep[MCMC; \eg][]{carnall2018, leja2019a, tacchella2021}.  These sampling techniques are essential for the efficient exploration given the relatively high dimensionality of SED model parameter space.  Even advanced techniques, however, are subject to major limitations.  For instance, MCMC can struggle to accurately estimate multimodal and degenerate posteriors.  Many techniques also require significant hand-tuning.  More importantly, despite their efficiency, these techniques require on the order of a {\em million} SED model evaluations to derive a posterior --- this can take ${\sim}10-100$ of CPU hours per galaxy.  Analyzing the tens of millions of spectra or billions of photometry from upcoming surveys (\eg~DESI, PFS, Rubin, JWST, Roman) with these approaches would thus require {\em billions of CPU hours}.

% overview of SBI and mention of ABC
Simulation-based inference (SBI; also known as ``likelihood-free'' inference)
offers an alternative that requires no assumptions about the form of the
likelihood. 
Instead, SBI uses a generative model, \emph{i.e.} a simulation $F$, to generate
mock data $\bfi{x}'$ given parameters $\btheta'$: $F(\btheta') = \bfi{x}'$. 
It uses a large number of simulated pairs $(\btheta', \bfi{x}')$ to directly estimate
either the posterior  $p(\btheta\given \bfi{x})$, the likelihood
$p(\bfi{x}\given \btheta)$, or the joint distribution of the parameters and data $p(\btheta, \bfi{x})$. 
SBI has already been successfully applied to a number of Bayesian parameter
inference problems in astronomy~\citep[\emph{e.g.}][]{cameron2012, weyant2013,
hahn2017b, kacprzak2018, alsing2018, wong2020, huppenkothen2021, zhang2021}
and in physics~\citep[\emph{e.g.}][]{brehmer2019, cranmer2020}.

%One simple and pedagogical example of SBI is Approximate Bayesian Computation~\citep[ABC;][]{rubin1984, pritchard1999, beaumont2002}, which uses a rejection sampling framework to estimate the posterior.  First, parameter values are sampled from the prior: $\btheta'\sim p(\btheta)$.  The forward model, $F$, is then run on $\btheta'$ to generate simulated data $F(\btheta') = \bfi{x}'$.  If the simulated $\bfi{x}'$ is `close' to the observed $\bfi{x}$, usually based on a threshold on some distance criterion $\rho(\bfi{x}', \bfi{x}) < \epsilon$, $\btheta'$ is kept.  Otherwise, $\btheta'$ is rejected.  This process is repeated until there are enough samples to estimate the posterior.  The estimated posterior from ABC can be written as $p(\btheta \given \rho(F(\btheta), \bfi{x}) < \epsilon)$.  In the case where $\epsilon\rightarrow 0$, the conditional statement is equivalent to the condition $F(\btheta) = \bfi{x}$; thus, the estimated ABC posterior is  equivalent to the true posterior: $p(\btheta \given \rho(F(\btheta), \bfi{x}) < \epsilon\rightarrow 0) \equiv p(\btheta \given \bfi{x})$.


\subsection{Amortized Neural Posterior Estimation} \label{sec:flow}
SBI provides another a critical advantage over MCMC
inference methods --- it enables \emph{amortized inference}. 
For SED modeling using MCMC, each galaxy requires >$10^5$ model evaluations to
accurately estimate $p(\btheta \given \bfi{x})$~(\citealt{hahn2022}, Kwon~\etal~in
prep.).
In particular, model evaluations for calculating the posterior of one galaxy cannot
be used for another. 
That aspect is what make MCMC approaches for SED modeling of upcoming surveys
computationally infeasible.

%ABC produces unbiased estimates of the posterior and only requires a forward model of the observed data.  It makes no assumptions on the likelihood and, therefore, relaxes the assumptions that go into surrogate likelihood methods.  Nevertheless, ABC is based on rejection sampling and thus requires comparable number of model evaluations as standard MCMC sampling based techniques.  ABC is only the simplest SBI method; new SBI methods can infer posteriors with much fewer model evaluations.  Density estimation-based SBI methods~\citep[\eg][]{papamakarios2017, alsing2018, hahn2019c, greenberg2019, tejero-cantero2020}, for instance, use model evaluations to fit the $p(\btheta\given\bfi{x})$, $p(\bfi{x}\given \btheta$, or $p(\btheta, \bfi{x})$ probability distributions.  They can exploit recent advances in neural density estimation (NDE) that increasingly enable high-fidelity density estimation with fewer samples of the distribution.  For instance, the NDE in \cite{papamakarios2017} accurately estimates the $28\times28=784$-dimensional distribution of the MNIST dataset\footnote{\url{http://yann.lecun.com/exdb/mnist/}} with only tens of thousands of samples. 


With density estimation SBI, we require a large number (${\sim}10^6$) of model evaluations 
only initially to train an NDE, i.e. a neural network with parameters $\bphi$ that
is trained to estimate the density $p_\phi(\btheta \given \bfi{x}')$.
If the training covers the entire or at least the practically relevant portions of the
spaces of $\btheta$ and $\bfi{x}$, respectively, we can afterwards evaluate 
$p_\phi(\btheta\given\bfi{x}_i)$ for each galaxy $i$ with minimal
computational cost. 
The inference is therefore amortized and no additional model evaluations are
needed to generate the posterior for each galaxy.
This technique is called  Amortized Neural Posterior Estimation (ANPE) 
and has recently been applied to a broad range of astronomical applications
from analyzing gravitational waves~\citep[\emph{e.g.}][]{wong2020,dax2021} to
binary microlensing lensing~\citep{zhang2021}.
For SED modeling, the choice in favor of amortization is easy: the 
entire upfront cost for ANPE model evaluations would yield posteriors for only tens of galaxies with MCMC.

However, ANPE makes two important assumptions.
\begin{compactenum}
\item The simulator $F$ is capable of generating mock data $\bfi{x}'$ that is practically indistinguishable from the observations.
In terms of the expected signal, i.e. $m$ in Eq.~\ref{eq:likelihood}, it shares that requirement with any probabilistic modeling approach. But unlike likelihood-based evaluations such as conventional MCMC, data generated for SBI need to include all relevant noise terms as well. We will address these two aspects in sections \ref{sec:training} and \ref{sec:forward-model}
\item The NDE is trained well, so that $p_\phi(\btheta \given \bfi{x}')$ is a good approximation of $p(\btheta \given \bfi{x}')$, and therefore, per item 1, of $p(\btheta \given \bfi{x})$. We will assess this question in section~\ref{sec:results}.
\end{compactenum}

ANPE commonly employs so-called ``normalizing flows''~\citep{tabak2010,
tabak2013} as density estimators.
Normalizing flow models use an invertible bijective transformation, $f$, to map
a complex target distribution to a simple base distribution, $\pi(\bfi{z})$, that is
fast to evaluate.
For ANPE, the target distribution is $p(\btheta \given \bfi{x})$ and the
$\pi(\bfi{z})$ is typically a simple multivariate Gaussian, or mixture of Gaussians.
The transformation $f: \bfi{z} \rightarrow \btheta$ must be invertible and have a
tractable Jacobian. 
This is so that we can evaluate the target distribution from $\pi(z)$ by a 
change of variable:  
\begin{equation} \label{eq:normflow}
    p(\btheta \given {\bf x}) = \pi(\bfi{z}) \Bigl|{\rm det} \left(\frac{\partial
    f^{-1}}{\partial \btheta} \right)\Bigr|.
\end{equation} 
Since the base distribution is easy to evaluate, we can also easily evaluate
the target distribution.  
A neural network is trained to obtain $f$, the collection of its parameters form $\bphi$.
The network typically consists of a series of simple transforms (\emph{e.g.}
shift and scale transforms) that are each invertible and whose Jacobians are
easily calculated. 
By stringing together many such transforms, $f$ provides an extremely flexible
mapping from the base distribution.
%Rather than a single complicated transformation, the network is typically restricted to a series of simple transforms that are each invertible and whose Jacobians are easily calculated. 

Many different normalizing flow models are now available in the
literature~\citep[\emph{e.g.}][]{germain2015, durkan2019}.
In this work, we use Masked Autoregressive
Flow~\citep[MAF;][]{papamakarios2017}. 
The autoregressive design~\citep{uria2016} of MAF is particularly well-suited
for modeling conditional probability distributions such as the posterior. 
Autoregressive models exploit chain rule to expand a joint probability of a set
of random variables as products of one-dimensional conditional
probabilities: $p(\bfi{x}) = \prod_i p(x_i\given x_{1:i-1})$. 
They then use neural networks to describe each conditional probability,
$p(x_i\given x_{1:i-1})$. 
In this context, we can add a conditional variable $y$ on both sides of the
equation, $p(\bfi{x}\given \bfi{y}) = \prod_i p(x_i\given x_{1:i-1}, \bfi{y})$, so that the
autoregressive model describes a conditional probability $p(\bfi{x}\given \bfi{y})$. 
One drawback of autoregressive models is their sensitivity to the ordering of
the variables. 
Masked Autoencoder for Distribution Estimation~\citep[MADE;][]{germain2015}
models address this limitation using binary masks to impose the autoregressive dependence and by permutating the order of the conditioning variables.
%This ensures that the MADE model is autoregressive and can be efficiently
%calculated. 
A MAF model is built by stacking multiple MADE models.  
Hence, it has the autoregressive structure of MADE but with more flexibility to
describe complex probability distributions.  
In practice, we use the MAF implementation in the $\mathtt{sbi}$ Python
package\footnote{\url{https://github.com/mackelab/sbi/}}~\citep{greenberg2019,
tejero-cantero2020}.
