\section{Simulation-Based Inference} \label{sec:sbi}
% standard bayesian approach and introducing SBI
The ultimate goal of Bayesian SED modeling, and probabilistic inference more
broadly, is to infer the posterior probability distributions of galaxy
properties, $\theta$, given observations, $\xobs$  --- $p(\theta\given\xobs)$.
We can evaluate the posterior at a specific $\theta$ and ${\bf x}$ using
Bayes' rule, $p(\theta\given\xobs) \propto p(\theta)~p(\xobs\given\theta)$. 
$p(\theta)$ is the prior distribution, which we specify. 
And $p(\xobs\given\theta)$ is the likelihood, which is {\em typically}
evaluated using a surrogate Gaussian functional form: 
\beq
    \ln p(\xobs\given\theta) = -\frac{1}{2}(\xobs - m(\theta))^t {\bf C}^{-1}
    (\xobs - m(\theta)).
\eeq
$m(\theta)$ is the theoretical model, in our case a galaxy SED model from
stellar population synthesis.
${\bf C}$ is the covariance matrix of the observations. 
In practice, off-diagonal terms are often ignored and measured are
uncertainties are used as estimates of the diagonal terms. 

In the standard approach, the full posterior distribution is derived by
evaluating the posterior with a sampling technique such as Markov Chain Monte
Carlo (MCMC) or nested sampling~\citep[\eg][]{carnall2017, leja2019a,
tacchella2021}.
These sampling techniques are essential for the efficient exploration of
the relatively higher dimensionality of SED model parameter space.
Even advanced techniques, however, are subject to major limitations.  
For instance, MCMC sampling techniques can struggle to accurately estimate
multimodal and degenerate posteriors. 
Many also require significant hand-tuning by the user.
More importantly, despite their efficiency, these techniques require on the
order of a {\em million} SED model evaluations to derive a posterior --- this
can take ${\sim}100$ of CPU hours per galaxy.
Analyzing the tens of millions of spectra or billions of photometry from
upcoming surveys (\eg~DESI, Rubin, Roman) with these approaches would thus
require {\em billions of CPU hours}.

% overview of SBI and mention of ABC
Simulation-based inference (SBI; also known as ``likelihood-free'' inference)
offers a more scalable approach to Bayesian SED modeling.
At its core, SBI involves any method that uses a forward model of the observed
data to directly estimate the posterior --- $p(\theta\given {\bf x})$, the
likelihood --- $p({\bf x}\given \theta)$, or the joint distribution of the
parameters and data --- $p(\theta, {\bf x})$. 
SBI methods have already been successfully applied to a number of Bayesian
parameter inference problems in astronomy~\citep[\emph{e.g.}][]{cameron2012, 
weyant2013, hahn2017b, kacprzak2018, alsing2018, wong2020, huppenkothen2021,
zhang2021}, and more broadly in physics~\citep[\emph{e.g.}][]{brehmer2019,
cranmer2020}

One simple and pedagogical example of SBI is Approximate Bayesian
Computation~\citep[ABC;][]{rubin1984, pritchard1999, beaumont2002}, which uses
a rejection sampling framework to estimate the posterior. 
First, parameter values are sampled from the prior: $\theta'\sim p(\theta)$. 
The forward model, $F$ is then run on the sampled $\theta'$ to generate
simulated data $F(\theta') = {\bf x}'$.
If the simulated ${\bf x}'$ is `close' to the observed $\xobs$, usually based
on a threshold on some distance metric $\rho({\bf x}', \xobs) < \epsilon$, 
$\theta'$ is kept. 
Otherwise, $\theta'$ is rejected. 
This process is repeated until there are enough samples to estimate the
posterior. 
The estimated posterior from ABC can be written as 
$p(\theta \given \rho(F(\theta), \xobs) < \epsilon)$. 
In the case where $\epsilon\rightarrow 0$, the conditional statement is
equivalent to the condition $F(\theta) = \xobs$; thus, the estimated ABC
posterior is {\em equivalent} to the true posterior:
$p(\theta \given \rho(F(\theta), \xobs) < \epsilon\rightarrow 0) \equiv
p(\theta \given \xobs)$.

ABC produces unbiased estimates of the posterior and only requires a forward
model of the observed data.
It makes no assumptions on the likelihood and, therefore, relaxes the
assumptions that go into surrogate likelihood methods. 
Nevertheless, ABC is based on rejection sampling and thus requires comparable
number of model evaluations as standard MCMC sampling based techniques. 
ABC is only the simplest SBI method; new SBI methods can infer posteriors with
much fewer model evaluations.
Density estimaiton-based SBI methods~\citep[\eg][]{papamakarios2017,
alsing2018, hahn2019c, greenberg2019, tejero-cantero2020}, for instance, use
model evaluations to fit estimates of $p(\theta\given\xobs)$, $p({\bf
x}\given \theta$, or $p(\theta, {\bf x})$ probability distributions.  
They can exploit recent advances in neural density estimation (NDE) that
increasingly enable high-fidelity density estimation with fewer samples of the
distribution.
For instance, the NDE in \cite{papamakarios2017} accurately estimates the
$28\times28=784$-dimensional distribution of the MNIST
dataset\footnote{\url{http://yann.lecun.com/exdb/mnist/}} with only tens of
thousands of samples. 

\subsection{Amortized Neural Posterior Estimation} \label{sec:flow}
Density estimation SBI provides a critical advantage over MCMC sampling-based
inference methods --- it enables \emph{amortized inference}. 
With SED modeling using MCMC sampling, each galaxy requires >$10^5$ model
evaluations to accurately estimate $p(\theta \given \xobs)$. 
Moreover, model evaluations for calculating the posterior of one galaxy cannot
be used for another, so for large galaxy surveys with >$10^6$
galaxies~\citep[\emph{e.g.}][]{ahumada2020} this would require >100 billion
model evalulations.
Upcoming galaxy surveys will observe orders of magnitude more galaxies
(\emph{e.g.} DESI, PFS, Rubin, Roman).
Bayesian SED modeling with MCMC sampling for these large galaxy surveys is
utterly \emph{computationally infeasible}.

On the other hand, if we use density estimation SBI for SED modeling, we do not
require a large number of model evaluations for each galaxy. 
An initial set of model evaluations (${\sim}10^6$) is necessary to train an NDE
to accurately estimate $\hat{p}(\theta \given \xobs)$ --- \emph{i.e.} Neural
Posterior Estimation (NPE).
Once trained, $\hat{p}(\theta \given \xobs)$ is over the full $\theta$ and
$\xobs$-space so we can sample $\hat{p}(\theta \given {\bf x}_{{\rm obs}, i})$
for each galaxy with minimal computational cost. 
Hence, the inference is now amortized and no additional model evaluations are
needed to generate the posterior for each galaxy. 
In total, SED modeling with SBI for >$10^6$ galaxies requires the same number
of model evaluations as analyzing tens of galaxies using the MCMC sampling. 

Amoritzed inference using density esimation SBI has recently been applied to a
broad range of astronomical applications from analyzing gravitational
waves~\citep[\emph{e.g.}][]{wong2020,dax2021} to binary microlensing
lensing~\citep{zhang2021}.
They primarily use a class of NDE called normalizing flows~\citep{tabak2010,
tabak2013}.
Normalizing flow models use an invertible bijective transformation, $f$, to map
a complex target distribution to a simple base distribution, $\pi(z)$, that is
fast to evaluate.
In the case of NPE, the target distribution is $p(\theta \given {\bf x})$ and
the $\pi(z)$ is typically a simple multivariate Gaussian, or mixutre of
Gaussians. 

The transformation $f: z \rightarrow \theta$ must be invertible and have a
tractable Jacobian. 
This is so that we can evaluate the target distribution from $\pi(z)$ using
change of variable:  
\begin{equation}
    p(\theta \given {\bf x}) = \pi(z) \Bigl|{\rm det} \left(\frac{\partial
    f^{-1}}{\partial \theta} \right)\Bigr|.
\end{equation} 
The base distribution is easy to evaluate so we can also easily evaluate the
target distribution.  
A neural network is trained to obtain $f$.
The network typically consists of a series of simple transforms (\emph{e.g.}
shift and scale transforms) that are each invertible and whose Jacobians are
easily calculated. 
By stringing together many transforms, $f$ provides an extremely flexible
mapping from the base distribution.
%Rather than a single complicated transformation, the network is typically restricted to a series of simple transforms that are each invertible and whose Jacobians are easily calculated. 
There are now many different normalizing flow models available in the
literature such as Masked Autoregressive for Distribution
Estimation~\citep[MADE;][]{germain2015} or Neural Spline
Flows~\citep{durkan2019}. 

In this work, we use Masked Autoregressive
Flow~\citep[MAF;][]{papamakarios2017}. 
The autoregressive design~\citep{uria2016} of MAF is particularly well-suited
for modeling conditional probability distributions \emph{i.e.} the posterior. 
% explain autoregressive
Autoregressive models exploit chain rule to expand a joint probability of a set
of random variables as products of one-dimensional conditional probabilities:
$p(x) = \prod_i p(x_i\given x_{1:i-1})$. 
In this context, we can add another conditional variable $y$ to get 
$p(x\given y) = \prod_i p(x_i\given x_{1:i-1}, y)$. 
% explain MADE 
MAF is built by stacking multiple MADEs. 
In practice, we use the MAF implementation in the $\mathtt{sbi}$ Python
package~\citep{greenberg2019, tejero-cantero2020}.
