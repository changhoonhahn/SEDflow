
@article{aihara2011,
  title = {The {{Eighth Data Release}} of the {{Sloan Digital Sky Survey}}: First {{Data}} from {{SDSS}}-{{III}}},
  shorttitle = {The {{Eighth Data Release}} of the {{Sloan Digital Sky Survey}}},
  author = {Aihara, Hiroaki and Allende Prieto, Carlos and An, Deokkeun and Anderson, Scott F. and Aubourg, {\'E}ric and Balbinot, Eduardo and Beers, Timothy C. and Berlind, Andreas A. and Bickerton, Steven J. and Bizyaev, Dmitry and Blanton, Michael R. and Bochanski, John J. and Bolton, Adam S. and Bovy, Jo and Brandt, W. N. and Brinkmann, J. and Brown, Peter J. and Brownstein, Joel R. and Busca, Nicolas G. and Campbell, Heather and Carr, Michael A. and Chen, Yanmei and Chiappini, Cristina and Comparat, Johan and Connolly, Natalia and Cortes, Marina and Croft, Rupert A. C. and Cuesta, Antonio J. and {da Costa}, Luiz N. and Davenport, James R. A. and Dawson, Kyle and Dhital, Saurav and Ealet, Anne and Ebelke, Garrett L. and Edmondson, Edward M. and Eisenstein, Daniel J. and Escoffier, Stephanie and Esposito, Massimiliano and Evans, Michael L. and Fan, Xiaohui and Femen{\'i}a Castell{\'a}, Bruno and {Font-Ribera}, Andreu and Frinchaboy, Peter M. and Ge, Jian and Gillespie, Bruce A. and Gilmore, G. and Gonz{\'a}lez Hern{\'a}ndez, Jonay I. and Gott, J. Richard and Gould, Andrew and Grebel, Eva K. and Gunn, James E. and Hamilton, Jean-Christophe and Harding, Paul and Harris, David W. and Hawley, Suzanne L. and Hearty, Frederick R. and Ho, Shirley and Hogg, David W. and Holtzman, Jon A. and Honscheid, Klaus and Inada, Naohisa and Ivans, Inese I. and Jiang, Linhua and Johnson, Jennifer A. and Jordan, Cathy and Jordan, Wendell P. and Kazin, Eyal A. and Kirkby, David and Klaene, Mark A. and Knapp, G. R. and Kneib, Jean-Paul and Kochanek, C. S. and Koesterke, Lars and Kollmeier, Juna A. and Kron, Richard G. and Lampeitl, Hubert and Lang, Dustin and Le Goff, Jean-Marc and Lee, Young Sun and Lin, Yen-Ting and Long, Daniel C. and Loomis, Craig P. and Lucatello, Sara and Lundgren, Britt and Lupton, Robert H. and Ma, Zhibo and MacDonald, Nicholas and Mahadevan, Suvrath and Maia, Marcio A. G. and Makler, Martin and Malanushenko, Elena and Malanushenko, Viktor and Mandelbaum, Rachel and Maraston, Claudia and Margala, Daniel and Masters, Karen L. and McBride, Cameron K. and McGehee, Peregrine M. and McGreer, Ian D. and M{\'e}nard, Brice and {Miralda-Escud{\'e}}, Jordi and Morrison, Heather L. and Mullally, F. and Muna, Demitri and Munn, Jeffrey A. and Murayama, Hitoshi and Myers, Adam D. and Naugle, Tracy and Neto, Angelo Fausti and Nguyen, Duy Cuong and Nichol, Robert C. and O'Connell, Robert W. and Ogando, Ricardo L. C. and Olmstead, Matthew D. and Oravetz, Daniel J. and Padmanabhan, Nikhil and {Palanque-Delabrouille}, Nathalie and Pan, Kaike and Pandey, Parul and P{\^a}ris, Isabelle and Percival, Will J. and Petitjean, Patrick and Pfaffenberger, Robert and Pforr, Janine and Phleps, Stefanie and Pichon, Christophe and Pieri, Matthew M. and Prada, Francisco and {Price-Whelan}, Adrian M. and Raddick, M. Jordan and Ramos, Beatriz H. F. and Reyl{\'e}, C{\'e}line and Rich, James and Richards, Gordon T. and Rix, Hans-Walter and Robin, Annie C. and {Rocha-Pinto}, Helio J. and Rockosi, Constance M. and Roe, Natalie A. and Rollinde, Emmanuel and Ross, Ashley J. and Ross, Nicholas P. and Rossetto, Bruno M. and S{\'a}nchez, Ariel G. and Sayres, Conor and Schlegel, David J. and Schlesinger, Katharine J. and Schmidt, Sarah J. and Schneider, Donald P. and Sheldon, Erin and Shu, Yiping and Simmerer, Jennifer and Simmons, Audrey E. and Sivarani, Thirupathi and Snedden, Stephanie A. and Sobeck, Jennifer S. and Steinmetz, Matthias and Strauss, Michael A. and Szalay, Alexander S. and Tanaka, Masayuki and Thakar, Aniruddha R. and Thomas, Daniel and Tinker, Jeremy L. and Tofflemire, Benjamin M. and Tojeiro, Rita and Tremonti, Christy A. and Vandenberg, Jan and Vargas Maga{\~n}a, M. and Verde, Licia and Vogt, Nicole P. and Wake, David A. and Wang, Ji and Weaver, Benjamin A. and Weinberg, David H. and White, Martin and White, Simon D. M. and Yanny, Brian and Yasuda, Naoki and Yeche, Christophe and Zehavi, Idit},
  year = {2011},
  month = apr,
  journal = {The Astrophysical Journal Supplement Series},
  volume = {193},
  pages = {29},
  issn = {0067-0049},
  doi = {10.1088/0067-0049/193/2/29},
  abstract = {The Sloan Digital Sky Survey (SDSS) started a new phase in 2008 August,  with new instrumentation and new surveys focused on Galactic structure and chemical evolution, measurements of the baryon oscillation feature in the clustering of galaxies and the quasar Ly{$\alpha$} forest, and a radial velocity search for planets around \textasciitilde 8000 stars. This paper describes the first data release of SDSS-III (and the eighth counting from the beginning of the SDSS). The release includes five-band imaging of roughly 5200 deg2 in the southern Galactic cap, bringing the total footprint of the SDSS imaging to 14,555 deg2, or over a third of the Celestial Sphere. All the imaging data have been reprocessed with an improved sky-subtraction algorithm and a final, self-consistent photometric recalibration and flat-field determination. This release also includes all data from the second phase of the Sloan Extension for Galactic Understanding and Exploration (SEGUE-2), consisting of spectroscopy of approximately 118,000 stars at both high and low Galactic latitudes. All the more than half a million stellar spectra obtained with the SDSS spectrograph have been reprocessed through an improved stellar parameter pipeline, which has better determination of metallicity for high-metallicity stars.},
  keywords = {atlases,catalogs,surveys},
  file = {/Users/chahah/Zotero/storage/S2ZVH2XJ/Aihara et al. - 2011 - The Eighth Data Release of the Sloan Digital Sky S.pdf}
}

@article{alsing2018,
  title = {Massive Optimal Data Compression and Density Estimation for Scalable, Likelihood-Free Inference in Cosmology},
  author = {Alsing, Justin and Wandelt, Benjamin and Feeney, Stephen},
  year = {2018},
  month = jan,
  journal = {arXiv:1801.01497 [astro-ph]},
  eprint = {1801.01497},
  eprinttype = {arxiv},
  primaryclass = {astro-ph},
  abstract = {Many statistical models in cosmology can be simulated forwards but have intractable likelihood functions. Likelihood-free inference methods allow us to perform Bayesian inference from these models using only forward simulations, free from any likelihood assumptions or approximations. Likelihood-free inference generically involves simulating mock data and comparing to the observed data; this comparison in data-space suffers from the curse of dimensionality and requires compression of the data to a small number of summary statistics to be tractable. In this paper we use massive optimal data compression to reduce the dimensionality of the data-space to just one number per parameter, providing a natural and optimal framework for summary statistic choice for likelihood-free inference. Secondly, we introduce density estimation likelihood-free inference, which learns a parameterized model for joint distribution of data and parameters, yielding both the parameter posterior and the model evidence. This approach is conceptually simple, requires less tuning than traditional Approximate Bayesian Computation approaches to likelihood-free inference and can give high-fidelity posteriors from orders of magnitude fewer forward simulations. As an additional bonus, it enables parameter inference and Bayesian model comparison simultaneously. We demonstrate density estimation likelihood-free inference with massive data compression on an analysis of the joint light-curve analysis supernova data. We show that high-fidelity posterior inference is possible for full-scale cosmological data analyses with as few as \$\textbackslash sim 10\^4\$ simulations, with substantial scope for further improvement, demonstrating the scalability of likelihood-free inference to large and complex cosmological datasets.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
  file = {/Users/chahah/Zotero/storage/F4FBTJAK/Alsing et al. - 2018 - Massive optimal data compression and density estim.pdf;/Users/chahah/Zotero/storage/YUDBNXSM/1801.html}
}

@article{beaumont2002,
  title = {Approximate {{Bayesian Computation}} in {{Population Genetics}}},
  author = {Beaumont, Mark A. and Zhang, Wenyang and Balding, David J.},
  year = {2002},
  month = dec,
  journal = {Genetics},
  volume = {162},
  number = {4},
  pages = {2025--2035},
  publisher = {{Genetics}},
  issn = {0016-6731, 1943-2631},
  abstract = {We propose a new method for approximate Bayesian statistical inference on the basis of summary statistics. The method is suited to complex problems that arise in population genetics, extending ideas developed in this setting by earlier authors. Properties of the posterior distribution of a parameter, such as its mean or density curve, are approximated without explicit likelihood calculations. This is achieved by fitting a local-linear regression of simulated parameter values on simulated summary statistics, and then substituting the observed summary statistics into the regression equation. The method combines many of the advantages of Bayesian statistical inference with the computational efficiency of methods based on summary statistics. A key advantage of the method is that the nuisance parameters are automatically integrated out in the simulation step, so that the large numbers of nuisance parameters that arise in population genetics problems can be handled without difficulty. Simulation results indicate computational and statistical efficiency that compares favorably with those of alternative methods previously proposed in the literature. We also compare the relative efficiency of inferences obtained using methods based on summary statistics with those obtained directly from the data using MCMC.},
  chapter = {Investigations},
  copyright = {Copyright \textcopyright{} 2002 by the Genetics Society of America},
  langid = {english},
  pmid = {12524368},
  file = {/Users/chahah/Zotero/storage/YBI855FX/Beaumont et al. - 2002 - Approximate Bayesian Computation in Population Gen.pdf;/Users/chahah/Zotero/storage/85QQBBJG/2025.html}
}

@article{blanton2011,
  title = {Improved {{Background Subtraction}} for the {{Sloan Digital Sky Survey Images}}},
  author = {Blanton, Michael R. and Kazin, Eyal and Muna, Demitri and Weaver, Benjamin A. and {Price-Whelan}, Adrian},
  year = {2011},
  month = jul,
  journal = {The Astronomical Journal},
  volume = {142},
  pages = {31},
  issn = {0004-6256},
  doi = {10.1088/0004-6256/142/1/31},
  abstract = {We describe a procedure for background subtracting Sloan Digital Sky Survey (SDSS) imaging that improves the resulting detection and photometry of large galaxies on the sky. Within each SDSS drift scan run, we mask out detected sources and then fit a smooth function to the variation of the sky background. This procedure has been applied to all SDSS-III Data Release 8 images, and the results are available as part of that data set. We have tested the effect of our background subtraction on the photometry of large galaxies by inserting fake galaxies into the raw pixels, reanalyzing the data, and measuring them after background subtraction. Our technique results in no size-dependent bias in galaxy fluxes up to half-light radii r 50 \textasciitilde{} 100 arcsec; in contrast, for galaxies of that size the standard SDSS photometric catalog underestimates fluxes by about 1.5 mag. Our results represent a substantial improvement over the standard SDSS catalog results and should form the basis of any analysis of nearby galaxies using the SDSS imaging data.},
  keywords = {atmospheric effects,galaxies: photometry,methods: data analysis,techniques: image processing},
  file = {/Users/chahah/Zotero/storage/FJ9HWCKC/Blanton et al. - 2011 - Improved Background Subtraction for the Sloan Digi.pdf}
}

@article{brehmer2019,
  title = {Mining Gold from Implicit Models to Improve Likelihood-Free Inference},
  author = {Brehmer, Johann and Louppe, Gilles and Pavez, Juan and Cranmer, Kyle},
  year = {2019},
  month = aug,
  journal = {arXiv:1805.12244 [hep-ph, physics:physics, stat]},
  eprint = {1805.12244},
  eprinttype = {arxiv},
  primaryclass = {hep-ph, physics:physics, stat},
  abstract = {Simulators often provide the best description of real-world phenomena. However, they also lead to challenging inverse problems because the density they implicitly define is often intractable. We present a new suite of simulation-based inference techniques that go beyond the traditional Approximate Bayesian Computation approach, which struggles in a high-dimensional setting, and extend methods that use surrogate models based on neural networks. We show that additional information, such as the joint likelihood ratio and the joint score, can often be extracted from simulators and used to augment the training data for these surrogate models. Finally, we demonstrate that these new techniques are more sample efficient and provide higher-fidelity inference than traditional methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,High Energy Physics - Phenomenology,Physics - Data Analysis; Statistics and Probability,Statistics - Machine Learning},
  file = {/Users/chahah/Zotero/storage/NF9FRYY9/Brehmer et al. - 2019 - Mining gold from implicit models to improve likeli.pdf;/Users/chahah/Zotero/storage/9PSWFZ4L/1805.html}
}

@article{cameron2012,
  title = {Approximate {{Bayesian Computation}} for Astronomical Model Analysis: A Case Study in Galaxy Demographics and Morphological Transformation at High Redshift},
  shorttitle = {Approximate {{Bayesian Computation}} for Astronomical Model Analysis},
  author = {Cameron, E. and Pettitt, A. N.},
  year = {2012},
  month = sep,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {425},
  pages = {44--65},
  issn = {0035-8711},
  doi = {10.1111/j.1365-2966.2012.21371.x},
  abstract = {'Approximate Bayesian Computation' (ABC) represents a powerful  methodology for the analysis of complex stochastic systems for which the likelihood of the observed data under an arbitrary set of input parameters may be entirely intractable - the latter condition rendering useless the standard machinery of tractable likelihood-based, Bayesian statistical inference [e.g. conventional Markov chain Monte Carlo (MCMC) simulation]. In this paper, we demonstrate the potential of ABC for astronomical model analysis by application to a case study in the morphological transformation of high-redshift galaxies. To this end, we develop, first, a stochastic model for the competing processes of merging and secular evolution in the early Universe, and secondly, through an ABC-based comparison against the observed demographics of massive (Mgal {$>$} 1011 M{$\odot$}) galaxies (at 1.5 {$<$} z {$<$} 3) in the Cosmic Assembly Near-IR Deep Extragalatic Legacy Survey (CANDELS)/Extended Groth Strip (EGS) data set we derive posterior probability densities for the key parameters of this model. The 'Sequential Monte Carlo' implementation of ABC exhibited herein, featuring both a self-generating target sequence and self-refining MCMC kernel, is amongst the most efficient of contemporary approaches to this important statistical algorithm. We highlight as well through our chosen case study the value of careful summary statistic selection, and demonstrate two modern strategies for assessment and optimization in this regard. Ultimately, our ABC analysis of the high-redshift morphological mix returns tight constraints on the evolving merger rate in the early Universe and favours major merging (with disc survival or rapid reformation) over secular evolution as the mechanism most responsible for building up the first generation of bulges in early-type discs.},
  keywords = {galaxies: evolution,galaxies: formation,methods: statistical},
  file = {/Users/chahah/Zotero/storage/UCB9TMNL/Cameron and Pettitt - 2012 - Approximate Bayesian Computation for astronomical .pdf}
}

@article{carnall2017,
  title = {Inferring the Star-Formation Histories of Massive Quiescent Galaxies with {{BAGPIPES}}: Evidence for Multiple Quenching Mechanisms},
  shorttitle = {Inferring the Star-Formation Histories of Massive Quiescent Galaxies with {{BAGPIPES}}},
  author = {Carnall, A. C. and McLure, R. J. and Dunlop, J. S. and Dav{\'e}, R.},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.04452 [astro-ph]},
  eprint = {1712.04452},
  eprinttype = {arxiv},
  primaryclass = {astro-ph},
  abstract = {We present Bayesian Analysis of Galaxies for Physical Inference and Parameter EStimation, or BAGPIPES, a new Python tool which can be used to rapidly generate complex model galaxy spectra and to fit these to arbitrary combinations of spectroscopic and photometric data using the MultiNest algorithm. We extensively test our ability to recover realistic star-formation histories (SFHs) with BAGPIPES by fitting mock observations of quiescent galaxies from the MUFASA simulation. We show that a double-power-law model produces better agreement with realistic SFHs than an exponentially-declining model. We then perform a detailed analysis of the SFHs of a sample of 9312 quiescent galaxies from UltraVISTA with stellar masses, \$M\_* {$>$} 10\^\{10\}\textbackslash{} \textbackslash mathrm\{M\_\textbackslash odot\}\$ and redshifts \$0.25 {$<$} z\_\textbackslash mathrm\{obs\} {$<$} 3.75\$. The majority of our quiescent sample exhibit SFHs which rise gradually then quench relatively rapidly, over \$\textbackslash sim 1\{-\}2\$ Gyr. This behaviour is consistent with recent cosmological hydrodynamic simulations, where AGN-driven feedback in the low-accretion (jet) mode is the dominant quenching mechanism. In addition, we identify two further subsets of objects with distinct SFH shapes. At \$z\_\textbackslash mathrm\{obs\} \textbackslash gtrsim 1\$ we find a class of objects with SFHs which rise and fall very rapidly, with quenching timescales of \$\textbackslash lesssim 1\$ Gyr. These objects are consistent with (potentially merger-triggered) quasar-mode AGN feedback. Also, at \$z\_\textbackslash mathrm\{obs\} \textbackslash lesssim 1\$ we find a population with SFHs which quench more slowly than they rise, over \$\textbackslash gtrsim3\$ Gyr, which we speculate to be the result of a 'natural' quenching process, where galaxy SFHs die down gradually due to the diminishing overall cosmic gas supply. Purely passive evolution of the quiescent population at \$z\_\textbackslash mathrm\{obs\} \textbackslash gtrsim 0.5\$ is disfavoured by our results.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Astrophysics of Galaxies,Astrophysics - Instrumentation and Methods for Astrophysics},
  file = {/Users/chahah/Zotero/storage/KF3XDH4M/Carnall et al. - 2017 - Inferring the star-formation histories of massive .pdf;/Users/chahah/Zotero/storage/WTUEDEKQ/1712.html}
}

@article{carnall2018,
  title = {How to Measure Galaxy Star-Formation Histories {{I}}: Parametric Models},
  shorttitle = {How to Measure Galaxy Star-Formation Histories {{I}}},
  author = {Carnall, A. C. and Leja, J. and Johnson, B. D. and McLure, R. J. and Dunlop, J. S. and Conroy, C.},
  year = {2018},
  month = nov,
  journal = {arXiv:1811.03635 [astro-ph]},
  eprint = {1811.03635},
  eprinttype = {arxiv},
  primaryclass = {astro-ph},
  abstract = {Parametric models for galaxy star-formation histories (SFHs) are widely used, though they are known to impose strong priors on physical parameters. This has consequences for measurements of the galaxy stellar-mass function (GSMF), star-formation-rate density (SFRD) and star-forming main sequence (SFMS). We investigate the effects of the exponentially declining, delayed exponentially declining, lognormal and double power law SFH models using BAGPIPES. We demonstrate that each of these models imposes strong priors on specific star-formation rates (sSFRs), potentially biasing the SFMS, and also imposes a strong prior preference for young stellar populations. We show that stellar mass, SFR and mass-weighted age inferences from high-quality mock photometry vary with the choice of SFH model by at least 0.1, 0.3 and 0.2 dex respectively. However the biases with respect to the true values depend more on the true SFH shape than the choice of model. We also demonstrate that photometric data cannot discriminate between SFH models, meaning it is important to perform independent tests to find well-motivated priors. We finally fit a low-redshift, volume-complete sample of galaxies from the Galaxy and Mass Assembly (GAMA) Survey with each model. We demonstrate that our stellar masses and SFRs at redshift, \$z\textbackslash sim0.05\$ are consistent with other analyses. However, our inferred cosmic SFRDs peak at \$z\textbackslash sim0.4\$, approximately 6 Gyr later than direct observations suggest, meaning our mass-weighted ages are significantly underestimated. This makes the use of parametric SFH models for understanding mass assembly in galaxies challenging. In a companion paper we consider non-parametric SFH models.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Astrophysics of Galaxies,Astrophysics - Instrumentation and Methods for Astrophysics},
  file = {/Users/chahah/Zotero/storage/5842SKVT/Carnall et al. - 2018 - How to measure galaxy star-formation histories I .pdf;/Users/chahah/Zotero/storage/N9HT2VB7/1811.html}
}

@article{cheung2021,
  title = {Testing the Robustness of Simulation-Based Gravitational-Wave Population Inference},
  author = {Cheung, Damon H. T. and Wong, Kaze W. K. and Hannuksela, Otto A. and Li, Tjonnie G. F. and Ho, Shirley},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.06707 [astro-ph, physics:gr-qc]},
  eprint = {2112.06707},
  eprinttype = {arxiv},
  primaryclass = {astro-ph, physics:gr-qc},
  abstract = {Gravitational-wave population studies have become more important in gravitational-wave astronomy because of the rapid growth of the observed catalog. In recent studies, emulators based on different machine learning techniques are used to emulate the outcomes of the population synthesis simulation with fast speed. In this study, we benchmark the performance of two emulators that learn the truncated power-law phenomenological model by using Gaussian process regression and normalizing flows techniques to see which one is a more capable likelihood emulator in the population inference. We benchmark the characteristic of the emulators by comparing their performance in the population inference to the phenomenological model using mock and real observation data. Our results suggest that the normalizing flows emulator can recover the posterior distribution by using the phenomenological model in the population inference with up to 300 mock injections. The normalizing flows emulator also underestimates the uncertainty for some posterior distributions in the population inference on real observation data. On the other hand, the Gaussian process regression emulator has poor performance on the same task and can only be used effectively in low-dimension cases.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - High Energy Astrophysical Phenomena,Astrophysics - Instrumentation and Methods for Astrophysics,General Relativity and Quantum Cosmology},
  file = {/Users/chahah/Zotero/storage/JSD2JZ4G/Cheung et al. - 2021 - Testing the robustness of simulation-based gravita.pdf;/Users/chahah/Zotero/storage/5MURS4QC/2112.html}
}

@article{cranmer2020,
  title = {The Frontier of Simulation-Based Inference},
  author = {Cranmer, Kyle and Brehmer, Johann and Louppe, Gilles},
  year = {2020},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {48},
  eprint = {https://www.pnas.org/content/117/48/30055.full.pdf},
  pages = {30055--30062},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424},
  doi = {10.1073/pnas.1912789117},
  abstract = {Many domains of science have developed complex simulations to describe phenomena of interest. While these simulations provide high-fidelity models, they are poorly suited for inference and lead to challenging inverse problems. We review the rapidly developing field of simulation-based inference and identify the forces giving additional momentum to the field. Finally, we describe how the frontier is expanding so that a broad audience can appreciate the profound influence these developments may have on science.There are no data associated with this paper.}
}

@article{dax2021,
  title = {Real-Time Gravitational-Wave Science with Neural Posterior Estimation},
  author = {Dax, Maximilian and Green, Stephen R. and Gair, Jonathan and Macke, Jakob H. and Buonanno, Alessandra and Sch{\"o}lkopf, Bernhard},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.12594 [astro-ph, physics:gr-qc]},
  eprint = {2106.12594},
  eprinttype = {arxiv},
  primaryclass = {astro-ph, physics:gr-qc},
  abstract = {We demonstrate unprecedented accuracy for rapid gravitational-wave parameter estimation with deep learning. Using neural networks as surrogates for Bayesian posterior distributions, we analyze eight gravitational-wave events from the first LIGO-Virgo Gravitational-Wave Transient Catalog and find very close quantitative agreement with standard inference codes, but with inference times reduced from O(day) to a minute per event. Our networks are trained using simulated data, including an estimate of the detector-noise characteristics near the event. This encodes the signal and noise models within millions of neural-network parameters, and enables inference for any observed data consistent with the training distribution, accounting for noise nonstationarity from event to event. Our algorithm -- called "DINGO" -- sets a new standard in fast-and-accurate inference of physical parameters of detected gravitational-wave events, which should enable real-time data analysis without sacrificing accuracy.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Machine Learning,General Relativity and Quantum Cosmology},
  file = {/Users/chahah/Zotero/storage/ICLX53PJ/Dax et al. - 2021 - Real-time gravitational-wave science with neural p.pdf;/Users/chahah/Zotero/storage/FGSEN2XT/2106.html}
}

@misc{greenberg2019,
  title = {Automatic {{Posterior Transformation}} for {{Likelihood}}-{{Free Inference}}},
  author = {Greenberg, David S. and Nonnenmacher, Marcel and Macke, Jakob H.},
  year = {2019},
  month = may,
  journal = {arXiv e-prints},
  abstract = {How can one perform Bayesian inference on stochastic simulators with intractable likelihoods? A recent approach is to learn the posterior from adaptively proposed simulations using neural network-based conditional density estimators. However, existing methods are limited to a narrow range of proposal distributions or require importance weighting that can limit performance in practice. Here we present automatic posterior transformation (APT), a new sequential neural posterior estimation method for simulation-based inference. APT can modify the posterior estimate using arbitrary, dynamically updated proposals, and is compatible with powerful flow-based density estimators. It is more flexible, scalable and efficient than previous simulation-based inference techniques. APT can operate directly on high-dimensional time series and image data, opening up new applications for likelihood-free inference.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {ADS Bibcode: 2019arXiv190507488G},
  file = {/Users/chahah/Zotero/storage/YR94IYTS/Greenberg et al. - 2019 - Automatic Posterior Transformation for Likelihood-.pdf}
}

@article{hahn2017b,
  title = {Approximate {{Bayesian Computation}} in {{Large Scale Structure}}: Constraining the Galaxy-Halo Connection},
  shorttitle = {Approximate {{Bayesian Computation}} in {{Large Scale Structure}}},
  author = {Hahn, ChangHoon and Vakili, Mohammadjavad and Walsh, Kilian and Hearin, Andrew P. and Hogg, David W. and Campbell, Duncan},
  year = {2017},
  month = aug,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {469},
  number = {3},
  eprint = {1607.01782},
  eprinttype = {arxiv},
  pages = {2791--2805},
  issn = {0035-8711, 1365-2966},
  doi = {10.1093/mnras/stx894},
  abstract = {Standard approaches to Bayesian parameter inference in large scale structure assume a Gaussian functional form (chi-squared form) for the likelihood. This assumption, in detail, cannot be correct. Likelihood free inferences such as Approximate Bayesian Computation (ABC) relax these restrictions and make inference possible without making any assumptions on the likelihood. Instead ABC relies on a forward generative model of the data and a metric for measuring the distance between the model and data. In this work, we demonstrate that ABC is feasible for LSS parameter inference by using it to constrain parameters of the halo occupation distribution (HOD) model for populating dark matter halos with galaxies. Using specific implementation of ABC supplemented with Population Monte Carlo importance sampling, a generative forward model using HOD, and a distance metric based on galaxy number density, two-point correlation function, and galaxy group multiplicity function, we constrain the HOD parameters of mock observation generated from selected "true" HOD parameters. The parameter constraints we obtain from ABC are consistent with the "true" HOD parameters, demonstrating that ABC can be reliably used for parameter inference in LSS. Furthermore, we compare our ABC constraints to constraints we obtain using a pseudo-likelihood function of Gaussian form with MCMC and find consistent HOD parameter constraints. Ultimately our results suggest that ABC can and should be applied in parameter inference for LSS analyses.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
  file = {/Users/chahah/Zotero/storage/7PC9B39G/Hahn et al. - 2017 - Approximate Bayesian Computation in Large Scale St.pdf;/Users/chahah/Zotero/storage/HPNZWAZN/1607.html}
}

@article{hahn2019c,
  title = {Likelihood Non-{{Gaussianity}} in Large-Scale Structure Analyses},
  author = {Hahn, ChangHoon and Beutler, Florian and Sinha, Manodeep and Berlind, Andreas and Ho, Shirley and Hogg, David W.},
  year = {2019},
  month = may,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {485},
  pages = {2956--2969},
  issn = {0035-8711},
  doi = {10.1093/mnras/stz558},
  abstract = {Standard present-day large-scale structure (LSS) analyses make a major assumption in their Bayesian parameter inference - that the likelihood has a Gaussian form. For summary statistics currently used in LSS, this assumption, even if the underlying density field is Gaussian, cannot be correct in detail. We investigate the impact of this assumption on two recent LSS analyses: the Beutler et al. power spectrum multipole (P{$\mathscr{l}$}) analysis and the Sinha et al. group multiplicity function ({$\zeta$}) analysis. Using non-parametric divergence estimators on mock catalogues originally constructed for covariance matrix estimation, we identify significant non-Gaussianity in both the P{$\mathscr{l}$} and {$\zeta$} likelihoods. We then use Gaussian mixture density estimation and independent component analysis on the same mocks to construct likelihood estimates that approximate the true likelihood better than the Gaussian pseudo-likelihood. Using these likelihood estimates, we accurately estimate the true posterior probability distribution of the Beutler et al. and Sinha et al. parameters. Likelihood non-Gaussianity shifts the f{$\sigma$}8 constraint by -0.44{$\sigma$}, but otherwise does not significantly impact the overall parameter constraints of Beutler et al. For the {$\zeta$} analysis, using the pseudo-likelihood significantly underestimates the uncertainties and biases the constraints of the Sinha et al. halo occupation parameters. For log M\_1 and {$\alpha$}, the posteriors are shifted by +0.43{$\sigma$} and -0.51{$\sigma$} and broadened by 42\{\{ per cent\}\} and 66\{\{ per cent\}\}, respectively. The divergence and likelihood estimation methods we present provide a straightforward framework for quantifying the impact of likelihood non-Gaussianity and deriving more accurate parameter constraints.},
  keywords = {cosmological parameters,cosmology: observations,galaxies: statistics,large-scale structure of Universe,methods: data analysis,methods: statistical},
  file = {/Users/chahah/Zotero/storage/3PJRJ6RV/Hahn et al. - 2019 - Likelihood non-Gaussianity in large-scale structur.pdf}
}

@article{handley2019,
  title = {Maximum-{{Entropy Priors}} with {{Derived Parameters}} in a {{Specified Distribution}}},
  author = {Handley, Will and Millea, Marius},
  year = {2019},
  month = mar,
  journal = {Entropy},
  volume = {21},
  number = {3},
  eprint = {1804.08143},
  eprinttype = {arxiv},
  pages = {272},
  issn = {1099-4300},
  doi = {10.3390/e21030272},
  abstract = {We propose a method for transforming probability distributions so that parameters of interest are forced into a specified distribution. We prove that this approach is the maximum entropy choice, and provide a motivating example applicable to neutrino hierarchy inference.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics,Astrophysics - Instrumentation and Methods for Astrophysics,High Energy Physics - Phenomenology,Mathematics - Statistics Theory},
  file = {/Users/chahah/Zotero/storage/QKP49984/Handley and Millea - 2019 - Maximum-Entropy Priors with Derived Parameters in .pdf;/Users/chahah/Zotero/storage/4YA53UR5/1804.html}
}

@misc{huppenkothen2021,
  title = {Accurate {{X}}-Ray {{Timing}} in the {{Presence}} of {{Systematic Biases With Simulation}}-{{Based Inference}}},
  author = {Huppenkothen, D. and Bachetti, M.},
  year = {2021},
  month = apr,
  journal = {arXiv e-prints},
  abstract = {Because many of our X-ray telescopes are optimized towards observing faint sources, observations of bright sources like X-ray binaries in outburst are often affected by instrumental biases. These effects include dead time and photon pile-up, which can dramatically change the statistical inference of physical parameters from these observations. While dead time is difficult to take into account in a statistically consistent manner, simulating dead time-affected data is often straightforward. This structure makes the issue of inferring physical properties from dead time-affected observations fall into a class of problems common across many scientific disciplines. There is a growing number of methods to address them under the names of Approximate Bayesian Computation (ABC) or Simulation-Based Inference (SBI), aided by new developments in density estimation and statistical machine learning. In this paper, we introduce SBI as a principled way to infer variability properties from dead time-affected light curves. We use Sequential Neural Posterior Estimation to estimate the posterior probability for variability properties. We show that this method can recover variability parameters on simulated data even when dead time is variable, and present results of an application of this approach to NuSTAR observations of the galactic black hole X-ray binary GRS 1915+105.},
  keywords = {Astrophysics - High Energy Astrophysical Phenomena,Astrophysics - Instrumentation and Methods for Astrophysics},
  annotation = {ADS Bibcode: 2021arXiv210403278H},
  file = {/Users/chahah/Zotero/storage/W9NYAR5Q/Huppenkothen and Bachetti - 2021 - Accurate X-ray Timing in the Presence of Systemati.pdf}
}

@article{iyer2017,
  title = {Reconstruction of {{Galaxy Star Formation Histories}} through {{SED Fitting}}: The {{Dense Basis Approach}}},
  shorttitle = {Reconstruction of {{Galaxy Star Formation Histories}} through {{SED Fitting}}},
  author = {Iyer, Kartheik G. and Gawiser, Eric},
  year = {2017},
  month = apr,
  journal = {The Astrophysical Journal},
  volume = {838},
  number = {2},
  eprint = {1702.04371},
  eprinttype = {arxiv},
  pages = {127},
  issn = {1538-4357},
  doi = {10.3847/1538-4357/aa63f0},
  abstract = {We introduce the Dense Basis method for Spectral Energy Distribution (SED) fitting. It accurately recovers traditional SED parameters, including M\$\_*\$, SFR and dust attenuation, and reveals previously inaccessible information about the number and duration of star formation episodes and the timing of stellar mass assembly, as well as uncertainties in these quantities. This is done using basis Star Formation Histories (SFHs) chosen by comparing the goodness-of-fit of mock galaxy SEDs to the goodness-of-reconstruction of their SFHs. We train and validate the method using a sample of realistic SFHs at \$z =1\$ drawn from stochastic realisations, semi-analytic models, and a cosmological hydrodynamical galaxy formation simulation. The method is then applied to a sample of 1100 CANDELS GOODS-S galaxies at \$1},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Astrophysics of Galaxies},
  file = {/Users/chahah/Zotero/storage/YDAXZXZP/Iyer and Gawiser - 2017 - Reconstruction of Galaxy Star Formation Histories .pdf;/Users/chahah/Zotero/storage/VZPMY39Z/1702.html}
}

@article{kacprzak2018,
  title = {Accelerating {{Approximate Bayesian Computation}} with {{Quantile Regression}}: Application to Cosmological Redshift Distributions},
  shorttitle = {Accelerating {{Approximate Bayesian Computation}} with {{Quantile Regression}}},
  author = {Kacprzak, T. and Herbel, J. and Amara, A. and R{\'e}fr{\'e}gier, A.},
  year = {2018},
  month = feb,
  journal = {Journal of Cosmology and Astro-Particle Physics},
  volume = {2018},
  pages = {042},
  doi = {10.1088/1475-7516/2018/02/042},
  abstract = {Approximate Bayesian Computation (ABC) is a method to obtain a posterior distribution without a likelihood function, using simulations and a set of distance metrics. For that reason, it has recently been gaining popularity as an analysis tool in cosmology and astrophysics. Its drawback, however, is a slow convergence rate. We propose a novel method, which we call qABC, to accelerate ABC with Quantile Regression. In this method, we create a model of quantiles of distance measure as a function of input parameters. This model is trained on a small number of simulations and estimates which regions of the prior space are likely to be accepted into the posterior. Other regions are then immediately rejected. This procedure is then repeated as more simulations are available. We apply it to the practical problem of estimation of redshift distribution of cosmological samples, using forward modelling developed in previous work. The qABC method converges to nearly same posterior as the basic ABC. It uses, however, only 20\% of the number of simulations compared to basic ABC, achieving a fivefold gain in execution time for our problem. For other problems the acceleration rate may vary; it depends on how close the prior is to the final posterior. We discuss possible improvements and extensions to this method.},
  langid = {english},
  file = {/Users/chahah/Zotero/storage/4W6WZY2W/Kacprzak et al. - 2018 - Accelerating Approximate Bayesian Computation with.pdf;/Users/chahah/Zotero/storage/CCHF448Q/abstract.html}
}

@article{legin2021,
  title = {Simulation-{{Based Inference}} of {{Strong Gravitational Lensing Parameters}}},
  author = {Legin, Ronan and Hezaveh, Yashar and Levasseur, Laurence Perreault and Wandelt, Benjamin},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.05278 [astro-ph]},
  eprint = {2112.05278},
  eprinttype = {arxiv},
  primaryclass = {astro-ph},
  abstract = {In the coming years, a new generation of sky surveys, in particular, Euclid Space Telescope (2022), and the Rubin Observatory's Legacy Survey of Space and Time (LSST, 2023) will discover more than 200,000 new strong gravitational lenses, which represents an increase of more than two orders of magnitude compared to currently known sample sizes. Accurate and fast analysis of such large volumes of data under a statistical framework is therefore crucial for all sciences enabled by strong lensing. Here, we report on the application of simulation-based inference methods, in particular, density estimation techniques, to the predictions of the set of parameters of strong lensing systems from neural networks. This allows us to explicitly impose desired priors on lensing parameters, while guaranteeing convergence to the optimal posterior in the limit of perfect performance.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics},
  file = {/Users/chahah/Zotero/storage/ALLRYYLJ/Legin et al. - 2021 - Simulation-Based Inference of Strong Gravitational.pdf;/Users/chahah/Zotero/storage/HQMCSUWG/2112.html}
}

@article{leja2019,
  title = {How to {{Measure Galaxy Star Formation Histories}}. {{II}}. {{Nonparametric Models}}},
  author = {Leja, Joel and Carnall, Adam C. and Johnson, Benjamin D. and Conroy, Charlie and Speagle, Joshua S.},
  year = {2019},
  month = may,
  journal = {ApJ},
  volume = {876},
  number = {1},
  pages = {3},
  issn = {0004-637X},
  doi = {10.3847/1538-4357/ab133c},
  abstract = {Nonparametric star formation histories (SFHs) have long promised to be the ``gold standard'' for galaxy spectral energy distribution (SED) modeling as they are flexible enough to describe the full diversity of SFH shapes, whereas parametric models rule out a significant fraction of these shapes a priori. However, this flexibility is not fully constrained even with high-quality observations, making it critical to choose a well-motivated prior. Here, we use the SED-fitting code Prospector to explore the effect of different nonparametric priors by fitting SFHs to mock UV-IR photometry generated from a diverse set of input SFHs. First, we confirm that nonparametric SFHs recover input SFHs with less bias and return more accurate errors than do parametric SFHs. We further find that, while nonparametric SFHs robustly recover the overall shape of the input SFH, the primary determinant of the size and shape of the posterior star formation rate as a function of time (SFR(t)) is the choice of prior, rather than the photometric noise. As a practical demonstration, we fit the UV-IR photometry of{\~ }6000 galaxies from the Galaxy and Mass Assembly survey and measure scatters between priors to be 0.1 dex in mass, 0.8 dex in SFR\textsubscript{100 Myr}, and 0.2 dex in mass-weighted ages, with the bluest star-forming galaxies showing the most sensitivity. An important distinguishing characteristic for nonparametric models is the characteristic timescale for changes in SFR(t). This difference controls whether galaxies are assembled in bursts or in steady-state star formation, corresponding respectively to (feedback-dominated/accretion-dominated) models of galaxy formation and to (larger/smaller) confidence intervals derived from SED fitting. High-quality spectroscopy has the potential to further distinguish between these proposed models of SFR(t).},
  langid = {english},
  file = {/Users/chahah/Zotero/storage/HZPMS7B5/Leja et al. - 2019 - How to Measure Galaxy Star Formation Histories. II.pdf;/Users/chahah/Zotero/storage/WUZ8Q3VM/abstract.html}
}

@article{leja2019a,
  title = {A {{New Census}} of the 0.2 \&lt; z \&lt; 3.0 {{Universe}}, {{Part I}}: The {{Stellar Mass Function}}},
  shorttitle = {A {{New Census}} of the 0.2 \&lt; z \&lt; 3.0 {{Universe}}, {{Part I}}},
  author = {Leja, Joel and Speagle, Joshua S. and Johnson, Benjamin D. and Conroy, Charlie and {van Dokkum}, Pieter and Franx, Marijn},
  year = {2019},
  month = oct,
  journal = {arXiv},
  pages = {arXiv:1910.04168},
  abstract = {There has been a long-standing factor-of-two tension between the observed star formation rate density and the observed stellar mass buildup after \$z\textbackslash sim2\$. Recently we have proposed that sophisticated panchromatic SED models can resolve this tension, as these methods infer systematically higher masses and lower star formation rates than standard approaches. In a series of papers we now extend this analysis and present a complete, self-consistent census of galaxy formation over \$0.2 \&lt; z \&lt; 3\$ inferred with the \textbackslash texttt\{Prospector\} galaxy SED-fitting code. In this work, Paper I, we present the evolution of the galaxy stellar mass function using new mass measurements of \$\textbackslash sim\$10\$\^5\$ galaxies in the 3D-HST and COSMOS-2015 surveys. We employ a new methodology to infer the mass function from the observed stellar masses: instead of fitting independent mass functions in a series of fixed redshift intervals, we construct a continuity model that directly fits for the redshift evolution of the mass function. This approach ensures a smooth picture of galaxy assembly and makes use of the full, non-Gaussian uncertainty contours in our stellar mass inferences. The resulting mass function has higher number densities at a fixed stellar mass than almost any other measurement in the literature, largely owing to the older stellar ages inferred by \textbackslash texttt\{Prospector\}. The stellar mass density is \$\textbackslash sim\$50\% higher than previous measurements, with the offset peaking at \$z\textbackslash sim1\$. The next two papers in this series will present the new measurements of star-forming main sequence and the cosmic star formation rate density, respectively. {$<$}P /{$>$}},
  langid = {english},
  file = {/Users/chahah/Zotero/storage/4TV3QZSU/Leja et al. - A New Census of the 0.2  z  3.0 Universe, Part I.pdf;/Users/chahah/Zotero/storage/BEHHHLAD/abstract.html}
}

@article{papamakarios2017,
  title = {Masked {{Autoregressive Flow}} for {{Density Estimation}}},
  author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
  year = {2017},
  month = may,
  journal = {arXiv e-prints},
  volume = {1705},
  pages = {arXiv:1705.07057},
  abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/chahah/Zotero/storage/VCZVEKYM/Papamakarios et al. - 2017 - Masked Autoregressive Flow for Density Estimation.pdf}
}

@article{papamakarios2019,
  title = {Neural {{Density Estimation}} and {{Likelihood}}-Free {{Inference}}},
  author = {Papamakarios, George},
  year = {2019},
  month = oct,
  journal = {arXiv e-prints},
  volume = {1910},
  pages = {arXiv:1910.13233},
  abstract = {I consider two problems in machine learning and statistics: the problem of estimating the joint probability density of a collection of random variables, known as density estimation, and the problem of inferring model parameters when their likelihood is intractable, known as likelihood-free inference. The contribution of the thesis is a set of new methods for addressing these problems that are based on recent advances in neural networks and deep learning.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/chahah/Zotero/storage/86W9WXW2/Papamakarios - 2019 - Neural Density Estimation and Likelihood-free Infe.pdf}
}

@article{pritchard1999,
  title = {Population Growth of Human {{Y}} Chromosomes: A Study of {{Y}} Chromosome Microsatellites.},
  shorttitle = {Population Growth of Human {{Y}} Chromosomes},
  author = {Pritchard, J. K. and Seielstad, M. T. and {Perez-Lezaun}, A. and Feldman, M. W.},
  year = {1999},
  month = dec,
  journal = {Molecular Biology and Evolution},
  volume = {16},
  number = {12},
  pages = {1791--1798},
  publisher = {{Oxford Academic}},
  issn = {0737-4038},
  doi = {10.1093/oxfordjournals.molbev.a026091},
  abstract = {We use variation at a set of eight human Y chromosome microsatellite loci to investigate the demographic history of the Y chromosome. Instead of assuming a popu},
  langid = {english},
  file = {/Users/chahah/Zotero/storage/EPPMPKT7/Pritchard et al. - 1999 - Population growth of human Y chromosomes a study .pdf;/Users/chahah/Zotero/storage/WYX9ZUDH/2925409.html}
}

@article{rubin1984,
  title = {Bayesianly {{Justifiable}} and {{Relevant Frequency Calculations}} for the {{Applied Statistician}}},
  author = {Rubin, Donald B.},
  year = {1984},
  journal = {The Annals of Statistics},
  volume = {12},
  number = {4},
  pages = {1151--1172},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364},
  abstract = {A common reaction among applied statisticians is that the Bayesian statistician's energies in an applied problem must be directed at the a priori elicitation of one model specification from which an optimal design and all inferences follow automatically by applying Bayes's theorem to calculate conditional distributions of unknowns given knowns. I feel, however, that the applied Bayesian statistician's tool-kit should be more extensive and include tools that may be usefully labeled frequency calculations. Three types of Bayesianly justifiable and relevant frequency calculations are presented using examples to convey their use for the applied statistician.},
  file = {/Users/chahah/Zotero/storage/NTTQYKY7/Rubin - 1984 - Bayesianly Justifiable and Relevant Frequency Calc.pdf}
}

@article{ruiz-macias2021,
  title = {Characterizing the Target Selection Pipeline for the {{Dark Energy Spectroscopic Instrument Bright Galaxy Survey}}},
  author = {{Ruiz-Macias}, Omar and Zarrouk, Pauline and Cole, Shaun and Baugh, Carlton M. and Norberg, Peder and Lucey, John and Dey, Arjun and Eisenstein, Daniel J. and Doel, Peter and Gazta{\~n}aga, Enrique and Hahn, ChangHoon and Kehoe, Robert and Kitanidis, Ellie and Landriau, Martin and Lang, Dustin and Moustakas, John and Myers, Adam D. and Prada, Francisco and Schubnell, Michael and Weinberg, David H. and Wilson, M. J.},
  year = {2021},
  month = apr,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {502},
  pages = {4328--4349},
  issn = {0035-8711},
  doi = {10.1093/mnras/stab292},
  abstract = {We present the steps taken to produce a reliable and complete input  galaxy catalogue for the Dark Energy Spectroscopic Instrument (DESI) Bright Galaxy Survey (BGS) using the photometric Legacy Survey DR8 DECam. We analyse some of the main issues faced in the selection of targets for the DESI BGS, such as star-galaxy separation, contamination by fragmented stars and bright galaxies. Our pipeline utilizes a new way to select BGS galaxies using Gaia photometry and we implement geometrical and photometric masks that reduce the number of spurious objects. The resulting catalogue is cross-matched with the Galaxy And Mass Assembly (GAMA) survey to assess the completeness of the galaxy catalogue and the performance of the target selection. We also validate the clustering of the sources in our BGS catalogue by comparing with mock catalogues and the Sloan Digital Sky Survey (SDSS) data. Finally, the robustness of the BGS selection criteria is assessed by quantifying the dependence of the target galaxy density on imaging and other properties. The largest systematic correlation we find is a 7 per cent suppression of the target density in regions of high stellar density.},
  keywords = {catalogues,large-scale structure of Universe,surveys},
  file = {/Users/chahah/Zotero/storage/AN595BZQ/Ruiz-Macias et al. - 2021 - Characterizing the target selection pipeline for t.pdf}
}

@article{schlegel1997,
  title = {Maps of {{Dust IR Emission}} for {{Use}} in {{Estimation}} of {{Reddening}} and {{CMBR Foregrounds}}},
  author = {Schlegel, D. J. and Finkbeiner, D. P. and Davis, Marc},
  year = {1997},
  month = dec,
  volume = {191},
  pages = {87.04},
  abstract = {We present a full sky 100micron map that is a reprocessed composite of  the COBE/DIRBE and IRAS/ISSA maps, with the zodiacal foreground and confirmed point sources removed. We have constructed a map of the dust temperature, so that the 100micron map can be converted to a map proportional to dust column density. The dust temperature varies from 17 K to 21 K, which is modest but does modify the estimate of the dust column by a factor of 5. The result of these manipulations is a map with DIRBE-quality calibration and IRAS resolution. A wealth of filamentary detail is apparent on many different scales at all Galactic latitudes. In high latitude regions, the dust map correlates well with maps of HI emission, but deviations are significant. To generate the full sky dust maps, we must first remove zodiacal light contamination as well as a possible cosmic infrared background (CIB). For the 100micron map no signficant CIB is detected, but in the 140micron and 240micron maps, where the zodiacal contamination is weaker, we detect the CIB at surprisingly high flux levels of 30 +/- 8 \{nW/m\}(2/sr) at 140\textbackslash micron, and 16 \textbackslash pm 3.4 \{nW/m\}\^2/sr at 240micron (95\% confidence), which is an integrated flux \textasciitilde{} 2 times that extrapolated from optical galaxies in the Hubble Deep Field. The primary use of these maps is likely to be as a new estimator of Galactic extinction. To calibrate our maps, we assume a standard reddening law, and use the colors of elliptical galaxies. We demonstrate that the new maps are twice as accurate as the older Burstein-Heiles reddening estimates in regions of low and moderate reddening. The maps are expected to be significantly more accurate in regions of high reddening. These dust maps will also be useful for estimating millimeter emission that contaminates CMBR experiments and for estimating soft X-ray absorption.}
}

@article{tacchella2021,
  title = {Fast, {{Slow}}, {{Early}}, {{Late}}: Quenching {{Massive Galaxies}} at \$z\textbackslash sim0.8\$},
  shorttitle = {Fast, {{Slow}}, {{Early}}, {{Late}}},
  author = {Tacchella, Sandro and Conroy, Charlie and Faber, S. M. and Johnson, Benjamin D. and Leja, Joel and Barro, Guillermo and Cunningham, Emily C. and Deason, Alis J. and Guhathakurta, Puragra and Guo, Yicheng and Hernquist, Lars and Koo, David C. and McKinnon, Kevin and Rockosi, Constance M. and Speagle, Joshua S. and {van Dokkum}, Pieter and Yesuf, Hassen M.},
  year = {2021},
  month = feb,
  journal = {arXiv e-prints},
  volume = {2102},
  pages = {arXiv:2102.12494},
  abstract = {We investigate the stellar populations for a sample of 161 massive, mainly quiescent galaxies at \$\textbackslash langle z\_\{\textbackslash rm obs\} \textbackslash rangle=0.8\$ with deep Keck/DEIMOS rest-frame optical spectroscopy (HALO7D survey). With the fully Bayesian framework Prospector, we simultaneously fit the spectroscopic and photometric data with an advanced physical model (including non-parametric star-formation histories, emission lines, variable dust attenuation law, and dust and AGN emission) together with an uncertainty and outlier model. We show that both spectroscopy and photometry are needed to break the dust-age-metallicity degeneracy. We find a large diversity of star-formation histories: although the most massive (\$M\_\{\textbackslash star\}{$>$}2\textbackslash times10\^\{11\}\textasciitilde M\_\{\textbackslash odot\}\$) galaxies formed the earliest (formation redshift of \$z\_\{\textbackslash rm f\}\textbackslash approx5-10\$ with a short star-formation timescale of \$\textbackslash tau\_\{\textbackslash rm SF\}\textbackslash lesssim1\textasciitilde\textbackslash mathrm\{Gyr\}\$), lower-mass galaxies have a wide range of formation redshifts, leading to only a weak trend of \$z\_\{\textbackslash rm f\}\$ with \$M\_\{\textbackslash star\}\$. Interestingly, several low-mass galaxies with have formation redshifts of \$z\_\{\textbackslash rm f\}\textbackslash approx5-8\$. Star-forming galaxies evolve about the star-forming main sequence, crossing the ridgeline several times in their past. Quiescent galaxies show a wide range and continuous distribution of quenching timescales (\$\textbackslash tau\_\{\textbackslash rm quench\}\textbackslash approx0-5\textasciitilde\textbackslash mathrm\{Gyr\}\$) with a median of \$\textbackslash langle\textbackslash tau\_\{\textbackslash rm quench\}\textbackslash rangle=1.0\_\{-0.9\}\^\{+0.8\}\textasciitilde\textbackslash mathrm\{Gyr\}\$ and of quenching epochs of \$z\_\{\textbackslash rm quench\}\textbackslash approx0.8-5.0\$ (\$\textbackslash langle z\_\{\textbackslash rm quench\}\textbackslash rangle=1.3\_\{-0.4\}\^\{+0.7\}\$). This large diversity of quenching timescales and epochs points toward a combination of internal and external quenching mechanisms. In our sample, rejuvenation and "late bloomers" are uncommon. In summary, our analysis supports the "grow \& quench" framework and is consistent with a wide and continuously-populated diversity of quenching timescales.},
  keywords = {Astrophysics - Astrophysics of Galaxies},
  file = {/Users/chahah/Zotero/storage/AK999XTA/Tacchella et al. - 2021 - Fast, Slow, Early, Late Quenching Massive Galaxie.pdf}
}

@article{talts2020,
  title = {Validating {{Bayesian Inference Algorithms}} with {{Simulation}}-{{Based Calibration}}},
  author = {Talts, Sean and Betancourt, Michael and Simpson, Daniel and Vehtari, Aki and Gelman, Andrew},
  year = {2020},
  month = oct,
  journal = {arXiv:1804.06788 [stat]},
  eprint = {1804.06788},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Verifying the correctness of Bayesian computation is challenging. This is especially true for complex models that are common in practice, as these require sophisticated model implementations and algorithms. In this paper we introduce \textbackslash emph\{simulation-based calibration\} (SBC), a general procedure for validating inferences from Bayesian algorithms capable of generating posterior samples. This procedure not only identifies inaccurate computation and inconsistencies in model implementations but also provides graphical summaries that can indicate the nature of the problems that arise. We argue that SBC is a critical part of a robust Bayesian workflow, as well as being a useful tool for those developing computational algorithms and statistical software.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/chahah/Zotero/storage/RKKIQDLB/Talts et al. - 2020 - Validating Bayesian Inference Algorithms with Simu.pdf;/Users/chahah/Zotero/storage/4VT8DQJR/1804.html}
}

@article{teimoorinia2021,
  title = {Mapping the {{Diversity}} of {{Galaxy Spectra}} with {{Deep Unsupervised Machine Learning}}},
  author = {Teimoorinia, Hossen and Archinuk, Finn and Woo, Joanna and Shishehchi, Sara and Bluck, Asa F. L.},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.03425 [astro-ph]},
  eprint = {2112.03425},
  eprinttype = {arxiv},
  primaryclass = {astro-ph},
  abstract = {Modern spectroscopic surveys of galaxies such as MaNGA consist of millions of diverse spectra covering different regions of thousands of galaxies. We propose and implement a deep unsupervised machine learning method to summarize the entire diversity of MaNGA spectra onto a 15x15 map (DESOM-1), where neighbouring points on the map represent similar spectra. We demonstrate our method as an alternative to conventional full spectral fitting for deriving physical quantities, as well as their full probability distributions, much more efficiently than traditional resource-intensive Bayesian methods. Since spectra are grouped by similarity, the distribution of spectra onto the map for a single galaxy, i.e., its "fingerprint", reveals the presence of distinct stellar populations within the galaxy indicating smoother or episodic star-formation histories. We further map the diversity of galaxy fingerprints onto a second map (DESOM-2). Using galaxy images and independent measures of galaxy morphology, we confirm that galaxies with similar fingerprints have similar morphologies and inclination angles. Since morphological information was not used in the mapping algorithm, relating galaxy morphology to the star-formation histories encoded in the fingerprints is one example of how the DESOM maps can be used to make scientific inferences.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Astrophysics of Galaxies},
  file = {/Users/chahah/Zotero/storage/GLYZFCBF/Teimoorinia et al. - 2021 - Mapping the Diversity of Galaxy Spectra with Deep .pdf;/Users/chahah/Zotero/storage/2G7FRHB5/2112.html}
}

@article{tejero-cantero2020,
  title = {Sbi: A Toolkit for Simulation-Based Inference},
  shorttitle = {Sbi},
  author = {{Tejero-Cantero}, Alvaro and Boelts, Jan and Deistler, Michael and Lueckmann, Jan-Matthis and Durkan, Conor and Gon{\c c}alves, Pedro J. and Greenberg, David S. and Macke, Jakob H.},
  year = {2020},
  month = aug,
  journal = {Journal of Open Source Software},
  volume = {5},
  number = {52},
  pages = {2505},
  issn = {2475-9066},
  doi = {10.21105/joss.02505},
  abstract = {Tejero-Cantero et al., (2020). sbi: A toolkit for simulation-based inference. Journal of Open Source Software, 5(52), 2505, https://doi.org/10.21105/joss.02505},
  langid = {english},
  file = {/Users/chahah/Zotero/storage/W7ZTXV3T/Tejero-Cantero et al. - 2020 - sbi A toolkit for simulation-based inference.pdf;/Users/chahah/Zotero/storage/CNSAGFZM/joss.html}
}

@article{webb2020,
  title = {The {{GOGREEN}} Survey: Post-Infall Environmental Quenching Fails to Predict the Observed Age Difference between Quiescent Field and Cluster Galaxies at z {$>$} 1},
  shorttitle = {The {{GOGREEN}} Survey},
  author = {Webb, Kristi and Balogh, Michael L. and Leja, Joel and {van der Burg}, Remco F. J. and Rudnick, Gregory and Muzzin, Adam and Boak, Kevin and Cerulo, Pierluigi and Gilbank, David and Lidman, Chris and Old, Lyndsay J. and {Pintos-Castro}, Irene and McGee, Sean and Shipley, Heath and Biviano, Andrea and Chan, Jeffrey C. C. and Cooper, Michael and De Lucia, Gabriella and Demarco, Ricardo and Forrest, Ben and Jablonka, Pascale and Kukstas, Egidijus and McCarthy, Ian G. and McNab, Karen and Nantais, Julie and Noble, Allison and Poggianti, Bianca and Reeves, Andrew M. M. and Vulcani, Benedetta and Wilson, Gillian and Yee, Howard K. C. and Zaritsky, Dennis},
  year = {2020},
  month = nov,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {498},
  pages = {5317--5342},
  issn = {0035-8711},
  doi = {10.1093/mnras/staa2752},
  abstract = {We study the star formation histories (SFHs) and mass-weighted ages of  331 UVJ-selected quiescent galaxies in 11 galaxy clusters and in the field at 1 {$<$} z {$<$} 1.5 from the Gemini Observations of Galaxies in Rich Early ENvironments (GOGREEN) survey. We determine the SFHs of individual galaxies by simultaneously fitting rest-frame optical spectroscopy and broad-band photometry to stellar population models. We confirm that the SFHs are consistent with more massive galaxies having on average earlier formation times. Comparing galaxies found in massive clusters with those in the field, we find galaxies with M* {$<$} 1011.3 M{$\odot$} in the field have more extended SFHs. From the SFHs we calculate the mass-weighted ages, and compare age distributions of galaxies between the two environments, at fixed mass. We constrain the difference in mass-weighted ages between field and cluster galaxies to \$0.31\_\{\^\{-0.33\}\}\^\{\_\{+0.51\}\}\$ Gyr, in the sense that cluster galaxies are older. We place this result in the context of two simple quenching models and show that neither environmental quenching based on time since infall (without pre-processing) nor a difference in formation times alone can reproduce both the average age difference and relative quenched fractions. This is distinctly different from local clusters, for which the majority of the quenched population is consistent with having been environmentally quenched upon infall. Our results suggest that quenched population in galaxy clusters at z {$>$} 1 has been driven by different physical processes than those at play at z = 0.},
  keywords = {galaxies: clusters: general,galaxies: evolution},
  file = {/Users/chahah/Zotero/storage/KWG49RA9/Webb et al. - 2020 - The GOGREEN survey post-infall environmental quen.pdf}
}

@article{weyant2013,
  title = {Likelihood-Free {{Cosmological Inference}} with {{Type Ia Supernovae}}: Approximate {{Bayesian Computation}} for a {{Complete Treatment}} of {{Uncertainty}}},
  shorttitle = {Likelihood-Free {{Cosmological Inference}} with {{Type Ia Supernovae}}},
  author = {Weyant, Anja and Schafer, Chad and {Wood-Vasey}, W. Michael},
  year = {2013},
  month = feb,
  journal = {The Astrophysical Journal},
  volume = {764},
  pages = {116},
  issn = {0004-637X},
  doi = {10.1088/0004-637X/764/2/116},
  abstract = {Cosmological inference becomes increasingly difficult when complex  data-generating processes cannot be modeled by simple probability distributions. With the ever-increasing size of data sets in cosmology, there is an increasing burden placed on adequate modeling; systematic errors in the model will dominate where previously these were swamped by statistical errors. For example, Gaussian distributions are an insufficient representation for errors in quantities like photometric redshifts. Likewise, it can be difficult to quantify analytically the distribution of errors that are introduced in complex fitting codes. Without a simple form for these distributions, it becomes difficult to accurately construct a likelihood function for the data as a function of parameters of interest. Approximate Bayesian computation (ABC) provides a means of probing the posterior distribution when direct calculation of a sufficiently accurate likelihood is intractable. ABC allows one to bypass direct calculation of the likelihood but instead relies upon the ability to simulate the forward process that generated the data. These simulations can naturally incorporate priors placed on nuisance parameters, and hence these can be marginalized in a natural way. We present and discuss ABC methods in the context of supernova cosmology using data from the SDSS-II Supernova Survey. Assuming a flat cosmology and constant dark energy equation of state, we demonstrate that ABC can recover an accurate posterior distribution. Finally, we show that ABC can still produce an accurate posterior distribution when we contaminate the sample with Type IIP supernovae.},
  keywords = {cosmological parameters,methods: statistical},
  file = {/Users/chahah/Zotero/storage/PYMFC67P/Weyant et al. - 2013 - Likelihood-free Cosmological Inference with Type I.pdf}
}

@article{wong2020,
  title = {Gravitational Wave Population Inference with Deep Flow-Based Generative Network},
  author = {Wong, Kaze W. K. and Contardo, Gabriella and Ho, Shirley},
  year = {2020},
  month = jun,
  journal = {Physical Review D},
  volume = {101},
  number = {12},
  eprint = {2002.09491},
  eprinttype = {arxiv},
  pages = {123005},
  issn = {2470-0010, 2470-0029},
  doi = {10.1103/PhysRevD.101.123005},
  abstract = {We combine hierarchical Bayesian modeling with a flow-based deep generative network, in order to demonstrate that one can efficiently constraint numerical gravitational wave (GW) population models at a previously intractable complexity. Existing techniques for comparing data to simulation,such as discrete model selection and Gaussian process regression, can only be applied efficiently to moderate-dimension data. This limits the number of observable (e.g. chirp mass, spins.) and hyper-parameters (e.g. common envelope efficiency) one can use in a population inference. In this study, we train a network to emulate a phenomenological model with 6 observables and 4 hyper-parameters, use it to infer the properties of a simulated catalogue and compare the results to the phenomenological model. We find that a 10-layer network can emulate the phenomenological model accurately and efficiently. Our machine enables simulation-based GW population inferences to take on data at a new complexity level.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - High Energy Astrophysical Phenomena,Astrophysics - Instrumentation and Methods for Astrophysics,General Relativity and Quantum Cosmology},
  file = {/Users/chahah/Zotero/storage/MPJKFSWW/Wong et al. - 2020 - Gravitational wave population inference with deep .pdf;/Users/chahah/Zotero/storage/MBS5W4G3/2002.html}
}

@article{zhang2021,
  title = {Real-{{Time Likelihood}}-{{Free Inference}} of {{Roman Binary Microlensing Events}} with {{Amortized Neural Posterior Estimation}}},
  author = {Zhang, Keming and Bloom, Joshua S. and Gaudi, B. Scott and Lanusse, Francois and Lam, Casey and Lu, Jessica R.},
  year = {2021},
  month = feb,
  doi = {10.3847/1538-3881/abf42e},
  abstract = {Fast and automated inference of binary-lens, single-source (2L1S) microlensing events with sampling-based Bayesian algorithms (e.g., Markov Chain Monte Carlo; MCMC) is challenged on two fronts: high computational cost of likelihood evaluations with microlensing simulation codes, and a pathological parameter space where the negative-log-likelihood surface can contain a multitude of local minima that are narrow and deep. Analysis of 2L1S events usually involves grid searches over some parameters to locate approximate solutions as a prerequisite to posterior sampling, an expensive process that often requires human-in-the-loop domain expertise. As the next-generation, space-based microlensing survey with the Roman Space Telescope is expected to yield thousands of binary microlensing events, a new fast and automated method is desirable. Here, we present a likelihood-free inference (LFI) approach named amortized neural posterior estimation, where a neural density estimator (NDE) learns a surrogate posterior \$\textbackslash hat\{p\}(\textbackslash theta|x)\$ as an observation-parametrized conditional probability distribution, from pre-computed simulations over the full prior space. Trained on 291,012 simulated Roman-like 2L1S simulations, the NDE produces accurate and precise posteriors within seconds for any observation within the prior support without requiring a domain expert in the loop, thus allowing for real-time and automated inference. We show that the NDE also captures expected posterior degeneracies. The NDE posterior could then be refined into the exact posterior with a downstream MCMC sampler with minimal burn-in steps.},
  langid = {english},
  file = {/Users/chahah/Zotero/storage/GG4UEYAJ/Zhang et al. - 2021 - Real-Time Likelihood-Free Inference of Roman Binar.pdf;/Users/chahah/Zotero/storage/SANKQBV5/2102.html}
}


